[{"title":"Using a Persistent Local Queue in Node.js","description":"As we've seen in the previous article of this series, an in-memory queue can be useful in streamlining a work load and retrying by recovering from errors, but it's subject to some problems.\n\nIf, for a","summary":"<p>As we've seen in <a href=\"http://blog.yld.io/2016/05/10/introducing-queues\">the previous article of this series</a>, an in-memory queue can be useful in streamlining a work load and retrying by recovering from errors, but it's subject to some problems.</p>\n\n<p>If, for a period of time, the production of messages exceeds the worker capacity, the queue may</p>","date":"2016-06-02T19:29:56.000Z","pubdate":"2016-06-02T19:29:56.000Z","pubDate":"2016-06-02T19:29:56.000Z","link":"http://blog.yld.io/2016/06/02/using-a-persistent-local-queue-in-node-js/","guid":"d39db85d-6e33-4f8a-8bad-99a8ba0ed6fd","author":"Pedro Teixeira","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/06/work-queues_png-1-1.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Using a Persistent Local Queue in Node.js"},"rss:description":{"@":{},"#":"<p>As we've seen in <a href=\"http://blog.yld.io/2016/05/10/introducing-queues\">the previous article of this series</a>, an in-memory queue can be useful in streamlining a work load and retrying by recovering from errors, but it's subject to some problems.</p>\n\n<p>If, for a period of time, the production of messages exceeds the worker capacity, the queue may</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/06/02/using-a-persistent-local-queue-in-node-js/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"d39db85d-6e33-4f8a-8bad-99a8ba0ed6fd"},"dc:creator":{"@":{},"#":"Pedro Teixeira"},"rss:pubdate":{"@":{},"#":"Thu, 02 Jun 2016 19:29:56 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/06/work-queues_png-1-1.png","medium":"image"}},"content:encoded":{"@":{},"#":"As we've seen in the previous article of this series, an in-memory queue can be useful in streamlining a work load and retrying by recovering from errors, but it's subject to some problems.\n\nIf, for a period of time, the production of messages exceeds the worker capacity, the queue may start taking too much memory.\n\nAlso, if the Node.js process goes down, you will lose whatever pending work it had. This may or not pose a problem, depending on the application.\n\nBy having the work queue taken out of memory and put into persistent storage we can obviate these problems: by keeping only the current work in memory, we can absorb work peaks, as they will get written onto the disk. Also, when the Node.js process starts, any pending work is resumed.\n\nFor the sake of an example, let's say that you're building a home alarm system and want to relay every event (alarm switched off, alarm switched on, door opened, alert, etc.), into a remote service for storage and future audit purposes. This remote service may be down or unreachable at times, but you still want the events to eventually — sorry for the pun — get there.\n\nLet's then build a module that creates and exports a persistent queue where security events will be stored. This module will also be responsible for relaying these events into the remote service.\n\nevent_relay.js:\n\nvar level = require('level');  \nvar db = level('./db');\n\nvar Jobs = require('level-jobs');\n\nvar maxConcurrency = 1;  \nvar queue = Jobs(db, worker, maxConcurrency);\n\nmodule.exports = queue;\n\nfunction worker(event, cb) {  \n  sendEventToRemoteService(event, function(err) {\n    if (err) console.error('Error processing event %s: %s', event.id, err.message);\n    else console.log('event %s successfully relayed', event.id);\n    cb(err);\n  });\n}\n\n\nfunction sendEventToRemoteService(event, cb) {  \n  setTimeout(function() {\n    var err;\n    if (Math.random() &gt; 0.5) err = Error('something awful has happened');\n    cb(err);\n  }, 100);\n}\n\n\nHere we start off by creating a LevelDB database where all the events will be persistently stored, inside a folder named db inside the current directory:\n\nvar level = require('level');  \nvar db = level('./db');  \n\n\nLevelDB is a generic key-value store, but here we'll not use it directly; instead we'll hand it off to the level-jobs package, which implements a worker queue on top of a LevelDB database:\n\nvar Jobs = require('level-jobs');\n\nvar maxConcurrency = 1;  \nvar queue = Jobs(db, worker, maxConcurrency);  \n\n\nHere we're creating a job queue by providing the LevelDB database that we created earlier and a worker function, and defining a maximum concurrency of 1. The worker function is a function very similar to the async.queue worker function: it accepts a work unit as the first argument and a callback function as the second argument:\n\nfunction worker(event, cb) {  \n  sendEventToRemoteService(event, function(err) {\n    if (err) console.error('Error processing event %s: %s', event.id, err.message);\n    else console.log('event %s successfully relayed', event.id);\n    cb(err);\n  });\n}\n\n\nThe worker function simply tries to send the work unit (an event, in our case) to the remote event storage service using this sendEventToRemoteService function. If the sending fails for some reason, our callback is invoked with an error, which we propagate to the worker callback. When the worker callback is invoked with an error, level-jobs retries until succeeded (up to a pre-defined maximum number of attempts,  using a back-off algorithm internally as in the previous section).\n\nWe then simulate event-delivery errors with a 50% probability in a fake implementation of the sendEventToRemoteService function:\n\nfunction sendEventToRemoteService(event, cb) {  \n  setTimeout(function() {\n    var err;\n    if (Math.random() &gt; 0.5) err = Error('something awful has happened');\n    cb(err);\n  }, 100);\n}\n\n\nLet's then create an event producer so that we can test our event relay module:\n\nevent_producer.js:\n\nvar relay = require('./event_relay');\n\nfor(var i = 0 ; i &lt; 10; i ++) {  \n  relay.push({id: i, event: 'door opened', when: Date.now()});\n}\n\n\nand run it:\n\n$ node event_producer\nevent 0 successfully relayed  \nevent 1 successfully relayed  \nevent 2 successfully relayed  \nevent 3 successfully relayed  \nError processing event 4: something awful has happened  \nError processing event 4: something awful has happened  \nError processing event 4: something awful has happened  \nError processing event 4: something awful has happened  \nevent 4 successfully relayed  \nError processing event 5: something awful has happened  \nError processing event 5: something awful has happened  \nError processing event 5: something awful has happened  \nevent 5 successfully relayed  \nevent 6 successfully relayed  \nError processing event 7: something awful has happened  \nevent 7 successfully relayed  \nError processing event 8: something awful has happened  \nevent 8 successfully relayed  \nError processing event 9: something awful has happened  \nevent 9 successfully relayed  \n\n\nWe can see by the output that, even though some failed, all the events eventually got relayed. Also, by setting the concurrency to 1, we make sure that no more than one pending work unit is being processed at any given time, no matter what the job creation rate is, thus saving memory and streamlining the work load.\n\nNext article\n\nSo far we've seen the usefulness of creating a local queue for streamlining the work load and persisting the work throughout process restarts. However, depending on the type of work that you're doing, this approach still has some problems: if this work is somehow CPU-intensive, you may need to outsource the work to a set of external worker processes.\n\nIn the next article of this series we'll be looking at how we can use a queue service to distribute the load between several node processes.\n\n\n\nThis article was extracted from the Work Queues book, a book from the Node Patterns series."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Announcing the HospitalRun Lisbon hackathon","description":"We are proud to announce that we're helping organise and participating in a very important event for us: the HospitalRun hackathon, on 18th June in Lisbon.\n\nHospitalRun is an open source, offline-first modern EMR/HIS platform built","summary":"<p>We are proud to announce that we're helping organise and participating in a very important event for us: the <strong><a href=\"http://hospitalrun.io/lisbon/\">HospitalRun hackathon</a></strong>, on 18th June in Lisbon.</p>\n\n<p>HospitalRun is an open source, offline-first modern EMR/HIS platform built specifically for the developing world. The project aims to provide low-resource environments a</p>","date":"2016-05-12T09:24:04.000Z","pubdate":"2016-05-12T09:24:04.000Z","pubDate":"2016-05-12T09:24:04.000Z","link":"http://blog.yld.io/2016/05/12/announcing-a-hospitalrun-hackathon-in-lisbon/","guid":"f95d4f47-2e58-4a63-bba3-74c7b163d18b","author":"Igor Soarez","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Announcing the HospitalRun Lisbon hackathon"},"rss:description":{"@":{},"#":"<p>We are proud to announce that we're helping organise and participating in a very important event for us: the <strong><a href=\"http://hospitalrun.io/lisbon/\">HospitalRun hackathon</a></strong>, on 18th June in Lisbon.</p>\n\n<p>HospitalRun is an open source, offline-first modern EMR/HIS platform built specifically for the developing world. The project aims to provide low-resource environments a</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/05/12/announcing-a-hospitalrun-hackathon-in-lisbon/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"f95d4f47-2e58-4a63-bba3-74c7b163d18b"},"dc:creator":{"@":{},"#":"Igor Soarez"},"rss:pubdate":{"@":{},"#":"Thu, 12 May 2016 09:24:04 GMT"},"content:encoded":{"@":{},"#":"We are proud to announce that we're helping organise and participating in a very important event for us: the HospitalRun hackathon, on 18th June in Lisbon.\n\nHospitalRun is an open source, offline-first modern EMR/HIS platform built specifically for the developing world. The project aims to provide low-resource environments a freely-available service that brings the best cloud technology healthcare solution to areas of the world where Internet connectivity is variable. The platform is built using Ember, Node, and CouchDB, and takes the issue of usability extremely seriously.\n\n\n  It is time to actually make a difference.\n\n\nThere's more to being a developer than writing frameworks and helping companies make a profit. It is time we put our skills to good use. It is time to actually make a difference. To the benefit of the many instead of the few. \n\nWe are going to help, by doing what we do best — developing!\n\n\n\nLisbon isn't just a beatiful city, it is home to some of the top tech talent of Europe. It is also where a big part of our team is based. This is why the whole YLD team is gathering in Lisbon for this event, so that we can put our weight behind this project.\n\n\n  We are going to help, by doing what we do best — developing!\n\n\nWe all know that the magic happens when skilled people get their minds together behind a common objective. You always learn something new working in a group like this, but nothing really tops the feeling of accomplishment when the team hits the goal. Come be a part of that!\n\n\n\nHackathons can get quite chaotic sometimes, and there are usually competing teams. But for this event we want to maximise the outcomes. We want people to work together, making the most of their contributions. This requires a lot of organisation, and that's why we're setting up teams around different issues. Each team will focus on a broader issue and will have a lead, who will prepare in advance to help coordinate the team during the hackathon day. Joining a team will likely maximise your contributions. However, if you prefer, you could join in as a solo hacker, picking out individual issues from the project's GitHub repository. Alternatively, contact us about forming your own team around another issue — we'd love to help.\n\n\n  10AM June 18th @ Microsoft Lisboa\n\n\nJoin us!\n\nThe HospitalRun hackathon will take place on a Saturday, 18th of June from 10AM to 6PM at Microsoft, Rua Sinais de Fogo Lote 2.07.02, 1990–110 Lisboa. Register now!"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Introducing Queues in Node.js","description":"This is the first of a series of articles that will be diving into using work queues to manage asynchronous work in Node.js, extracted from the Node Patterns series.\n\nEnjoy!\n\n\n\nIt is common for applications to have","summary":"<p>This is the first of a series of articles that will be diving into using work queues to manage asynchronous work in Node.js, extracted from <a href=\"http://nodepatternsbooks.com/\">the Node Patterns series</a>.</p>\n\n<p>Enjoy!</p>\n\n<hr>\n\n<p>It is common for applications to have workloads that can be processed asynchronously from application flows. A common example</p>","date":"2016-05-10T10:51:00.000Z","pubdate":"2016-05-10T10:51:00.000Z","pubDate":"2016-05-10T10:51:00.000Z","link":"http://blog.yld.io/2016/05/10/introducing-queues/","guid":"1ae98d8f-7007-48a0-bac8-cd32fcc398b8","author":"Pedro Teixeira","comments":null,"origlink":null,"image":{},"source":{},"categories":["node.js","queues","patterns"],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/05/work-queues_png-1.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Introducing Queues in Node.js"},"rss:description":{"@":{},"#":"<p>This is the first of a series of articles that will be diving into using work queues to manage asynchronous work in Node.js, extracted from <a href=\"http://nodepatternsbooks.com/\">the Node Patterns series</a>.</p>\n\n<p>Enjoy!</p>\n\n<hr>\n\n<p>It is common for applications to have workloads that can be processed asynchronously from application flows. A common example</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/05/10/introducing-queues/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"1ae98d8f-7007-48a0-bac8-cd32fcc398b8"},"rss:category":[{"@":{},"#":"node.js"},{"@":{},"#":"queues"},{"@":{},"#":"patterns"}],"dc:creator":{"@":{},"#":"Pedro Teixeira"},"rss:pubdate":{"@":{},"#":"Tue, 10 May 2016 10:51:00 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/05/work-queues_png-1.png","medium":"image"}},"content:encoded":{"@":{},"#":"This is the first of a series of articles that will be diving into using work queues to manage asynchronous work in Node.js, extracted from the Node Patterns series.\n\nEnjoy!\n\n\n\nIt is common for applications to have workloads that can be processed asynchronously from application flows. A common example of this is sending an email. For instance, when a new user registers, you may need to send him a confirmation email to validate that the address the user just entered is actually theirs. This involves composing the message from a template; making a request to an email provider; parsing the result; handling any eventual errors that may occur; retrying, etc.. This flow may be too complex, error-prone or take too long to include it in the cycle of an HTTP server. But there is an alternative: instead, we may just insert a document into persistent storage where we describe that there is a pending message to be sent to this particular user. Another process may then pick it up and do the heavy lifting: templating, contacting the server, parsing the errors, and rescheduling the work if necessary.\n\nAlso, it is quite common for systems to have to integrate with other systems. I have worked on some projects that require a two-way synchronisation of user profiles between different systems: when a user updates the profile on one system, those changes need to be propagated to the other system, and vice versa. If you don't require strong consistency between the two systems, where having a small delay between the synchronisation of profile data may just be acceptable, this work can be done asynchronously, by another process.\n\nMore generally, it's a common pattern throughout systems to have a work queue that separates the work producers from the work consumers. The producers insert work into a work queue and the consumers pop work from the queue, performing the required tasks.\n\n\n\nThere are many reasons and advantages on using such a topology, such as:\n\n\nDecoupling work producers and work consumers\nMaking retry logic easier to implement\nDistributing work load throughout time\nDistributing work load throughout space (nodes)\nMaking asynchronous work\nMaking external systems easier to integrate (eventual consistency)\n\n\nLet's analyse some of them.\n\nIsolate\n\nSending an email is an action that many applications need to do. As an example, when a user changes his password, some applications kindly send an email informing the user that someone (hopefully not someone else) changed the password. Nowadays, sending an email is usually done via a third-party email provider service through a HTTP API call. What happens if this service is slow or unreachable? You don't want to have to roll back the password change just because an email couldn't be sent. And surely you don't want to crash the password-change request because a non-important part of the work involved in processing that request failed. Sending that email is something that can hopefully be done quickly after the password gets changed, but not at this price.\n\nRetry\n\nAlso, changing a password may mean that you will have to change the password for that user in two systems: a central user database and a legacy system. (I know, that's terrible, but I've seen this more than once — the real world can be a messy place.) What happens if the first succeeds but the second fails?\n\nIn both these cases you want to retry until you succeed: changing the password in the legacy system is an operation that can be repeated many times with the same outcome, and sending an email can be retried many times.\n\n\n  If, for instance, the legacy system manages to change the password but somehow fails to reply successfully, you still may retry later given that the operation is idempotent.\n  \n  Even non-idempotent operations can benefit from being handled by a work queue. For instance, you can insert a money transaction into the work queue: if you give each money transaction a universal unique identifier the system that will later receive the transaction request can make sure that no duplicate transactions occur.\n  \n  In this case you mainly have to worry that the work queue provides the necessary persistence guarantees: you want to minimise the risk of losing transactions if systems malfunction.\n\n\nDistribute and scale\n\nAnother reason why you may want to totally decouple work producers from work consumers is that you may need to scale the worker cluster: if the task involved consumes many resources, if it's CPU-heavy, or needs lots of RAM or other OS resources, you can scale that separately from the rest of your application by putting it behind a work queue.\n\nIn any application, some operations are heavier than others. This may introduce disparate work loads throughout nodes: a node that unfortunately handles too many concurrent work-heavy operations may become overburdened while other nodes sit idle. By using a work queue you can minimise this effect by spreading specific work evenly among workers.\n\nAnother effect that a work queue can have is to absorb work peaks: you may plan your work cluster for a given maximum capacity and make sure that capacity is never exceeded. If the amount of work rises tremendously for a short period of time, that work will be absorbed by the work queue, isolating the workers from the peak.\n\n\n  Monitoring the system plays an important role here: you should constantly monitor the work-queue length, work latency (the time it takes to complete a work task), the worker occupation, and capacity for deciding on the optimal minimal resources for a satisfactory operation latency during peak time.\n\n\nSurvive crashes\n\nIf you don't need any of the above, a good reason to have a persistent work queue is to survive crashes. Even if an in-memory queue on the same process would fit your application needs, persisting that queue makes your application more resilient to process restarts.\n\nBut enough about theory – let's jump into implementation.\n\nThe Simplest Case: an In-Memory Work Queue\n\nThe simplest work queue you can devise in Node.js is an in-memory queue. Implementing an in-memory queue here would be an academic exercise (which I can leave to the reader). Here we'll be using Async's queue primitive to build a work queue.\n\nLet's imagine that you're building this domotic application that interfaces with a hardware unit that controls your house. Your Node.js application talks to this unit using a serial port, and the wire protocol only accepts one pending command at a time.\n\nThis protocol can be encapsulated inside this domotic.js module that exports three functions:\n\n\n.connect() - to connect to the domotic module\n.command() - to send a command and wait for the response\n.disconnect() - to disconnect from the module\n\n\nHere is a simulation of such a module:\n\ndomotic.js:\n\nexports.connect = connect;  \nexports.command = command;  \nexports.disconnect = disconnect;\n\nfunction connect(cb) {  \n  setTimeout(cb, 100); // simulate connection\n}\n\nfunction command(cmd, options, cb) {  \n  if (succeeds()) {\n    setTimeout(cb, 100); // simulate command\n  } else {\n    setTimeout(function() {\n      var err = Error('error connecting');\n      err.code = 'ECONN';\n      cb(err);\n    }, 100);\n  }\n\n}\n\nfunction disconnect(cb) {  \n  if (cb) setTimeout(cb, 100); // simulate disconnection\n}\n\nfunction succeeds() {  \n  return Math.random() &gt; 0.5;\n}\n\n\n\n  Notice here that we're not interacting with any domotic module; we're simply faking it, calling the callbacks with success after 100 milliseconds.\n  \n  Also, the .command function simulates connection errors: if succeeds() returns false, the command fails with a connection error with a probability of 50% (our domotic serial connection is very error-prone). This allows us to test if our application is successfully reconnecting and retrying the command after such a connection failure.\n\n\nWe can then create another module that can issue commands behind a queue:\n\ndomotic_queue.js:\n\nvar async = require('async');  \nvar Backoff = require('backoff');  \nvar domotic = require('./domotic');\n\nvar connected = false;\n\nvar queue = async.queue(work, 1);\n\n\nfunction work(item, cb) {  \n  ensureConnected(function() {\n    domotic.command(item.command, item.options, callback);\n  });\n\n  function callback(err) {\n    if (err &amp;&amp; err.code == 'ECONN') {\n      connected = false;\n      work(item);\n    } else cb(err);\n  }\n}\n\n\n/// command\n\nexports.command = pushCommand;\n\n\nfunction pushCommand(command, options, cb) {  \n  var work = {\n    command: command,\n    options: options\n  };\n\n  console.log('pushing command', work);\n\n  queue.push(work, cb);\n}\n\nfunction ensureConnected(cb) {  \n  if (connected) {\n    return cb();\n  } else {\n    var backoff = Backoff.fibonacci();\n    backoff.on('backoff', connect);\n    backoff.backoff();\n  }\n\n  function connect() {\n    domotic.connect(connected);\n  }\n\n  function connected(err) {\n    if (err) {\n      backoff.backoff();\n    } else {\n      connected = true;\n      cb();\n    }\n  }\n}\n\n\n/// disconnect\n\nexports.disconnect = disconnect;\n\nfunction disconnect() {  \n  if (! queue.length()) {\n    domotic.disconnect();\n  } else {\n    console.log('waiting for the queue to drain before disonnecting');\n    queue.drain = function() {\n      console.log('disconnecting');\n      domotic.disconnect();\n    };\n  }\n}\n\n\nThere is a lot going on here – let's analyise it by pieces:\n\nvar async = require('async');  \nvar Backoff = require('backoff');  \nvar domotic = require('./domotic');  \n\n\nHere we're importing some packages:\n\n\nasync - which will provide us with the memory-queue implementation;\nbackoff - which will allow us to increase the intervals after each failed attempt to reconnect;\n./domotic - our domotic simulator module.\n\n\nWe start our module in the disconnected state:\n\nvar connected = false;  \n\n\nWe create our async queue:\n\nvar queue = async.queue(work, 1);  \n\n\nHere we're providing a worker function named work (defined further down in the code) and a maximum concurrency of 1. Since we have defined that our domotic module protocol only allows for one outstanding command at a time, we're enforcing that here.\n\nWe then define the worker function that will process the queue items, one at a time:\n\nfunction work(item, cb) {  \n  ensureConnected(function() {\n    domotic.command(item.command, item.options, callback);\n  });\n\n  function callback(err) {\n    if (err &amp;&amp; err.code == 'ECONN') {\n      connected = false;\n      work(item);\n    } else cb(err);\n  }\n}\n\n\nWhen our async queue chooses to pop another work item, it calls our work function, passing in the work item and a callback for for us to call when the work is finished.\n\nFor each work item, we're making sure we're connected. Once we are connected, we use the domotic module to issue the command, using the command and options attributes that we know will be present in the work item. As a last argument we pass a callback function that we conveniently named callback, which will be called once the command succeeds or fails.\n\nOn the callback we're explicitly handling the connection error case by setting the connected state to false and calling work again, which will retry the reconnection.\n\nIf, instead, no error happens, we have the current work item terminated by calling the work callback cb.\n\nfunction ensureConnected(cb) {  \n  if (connected) {\n    return cb();\n  } else {\n    var backoff = Backoff.fibonacci();\n    backoff.on('backoff', connect);\n    backoff.backoff();\n  }\n\n  function connect() {\n    domotic.connect(connected);\n  }\n\n  function connected(err) {\n    if (err) {\n      backoff.backoff();\n    } else {\n      connected = true;\n      cb();\n    }\n  }\n}\n\n\nOur ensureConnected function is then responsible for either invoking the callback if we're on the connected state, or trying to connect otherwise. When trying to connect, we use the backoff module to increase the interval between each re-connection attempt. Every time the domotic.connect function calls back with an error, we back off, which increases the interval before triggering the backoff event. When the backoff event triggers we try to connect. Once we successfully connect we invoke the cb callback; otherwise, we keep retrying.\n\nThis module exports a .command function:\n\n/// command\n\nexports.command = pushCommand;\n\n\nfunction pushCommand(command, options, cb) {  \n  var work = {\n    command: command,\n    options: options\n  };\n\n  console.log('pushing command', work);\n\n  queue.push(work, cb);\n}\n\n\nThis command simply compiles a work item and pushes it to the queue.\n\nFinally, this module also exports a .disconnect function:\n\n/// disconnect\n\nexports.disconnect = disconnect;\n\nfunction disconnect() {  \n  if (! queue.length()) {\n    domotic.disconnect();\n  } else {\n    console.log('waiting for the queue to drain before disonnecting');\n    queue.drain = function() {\n      console.log('disconnecting');\n      domotic.disconnect();\n    };\n  }\n}\n\n\nHere we're basically making sure that the queue is empty before calling the disconnect method on our domotic module. If the queue is not empty, we wait for it to drain before actually disconnecting.\n\n\n  Optionally, in the case where the queue is not yet drained, you could set a timeout, enforcing the disconnect after that.\n\n\nWe can then create a domotic client:\n\nclient.js:\n\nvar domotic = require('./domotic_queue');\n\nfor(var i = 0 ; i &lt; 20; i ++) {  \n  domotic.command('toggle light', i, function(err) {\n    if (err) throw err;\n    console.log('command finished');\n  });\n}\n\ndomotic.disconnect();  \n\n\nHere we're pushing 20 settime commands to our domotic module in parallel, also passing in a callback that will be called once each command is finished. If one of those commands errors, we simply handle it by throwing and interrupting execution.\n\nRight after pushing all commands we immediately disconnect, but hopefully our module will wait until all the commands have been executed before actually disconnecting.\n\nLet's try it from the command line:\n\n$ node client.js\npushing command { command: 'toggle light', options: 0 }  \npushing command { command: 'toggle light', options: 1 }  \npushing command { command: 'toggle light', options: 2 }  \npushing command { command: 'toggle light', options: 3 }  \npushing command { command: 'toggle light', options: 4 }  \npushing command { command: 'toggle light', options: 5 }  \npushing command { command: 'toggle light', options: 6 }  \npushing command { command: 'toggle light', options: 7 }  \npushing command { command: 'toggle light', options: 8 }  \npushing command { command: 'toggle light', options: 9 }  \npushing command { command: 'toggle light', options: 10 }  \npushing command { command: 'toggle light', options: 11 }  \npushing command { command: 'toggle light', options: 12 }  \npushing command { command: 'toggle light', options: 13 }  \npushing command { command: 'toggle light', options: 14 }  \npushing command { command: 'toggle light', options: 15 }  \npushing command { command: 'toggle light', options: 16 }  \npushing command { command: 'toggle light', options: 17 }  \npushing command { command: 'toggle light', options: 18 }  \npushing command { command: 'toggle light', options: 19 }  \nwaiting for the queue to drain before disonnecting  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ncommand finished  \ndisconnecting  \n\n\nHere we can see that all the commands are immediately pushed into the queue, and that the commands finish in sequence, separated by a random period of time. Finally, after all the commands are completed, the disconnection occurs.\n\nNext article\n\nIn the next article of this series we'll be looking at how we can survive process crashes and limit memory impact by persisting the work items.\n\n\n\nThis article was extracted from the Work Queues book, a book from the Node Patterns series."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Container Camp San Francisco 2016: Wrap-Up","description":"The fourth incarnation of Container Camp, and the second in San Francisco, took place on Friday April 15th. YLD were in attendance and we've put together this follow-up blog post for those who couldn't be","summary":"<p>The fourth incarnation of Container Camp, and the second in San Francisco, took place on Friday April 15th. <a href=\"https://yld.io\">YLD</a> were in attendance and we've put together this follow-up blog post for those who couldn't be there.</p>\n\n<p>One of the great things about Container Camp has always been the broad representation</p>","date":"2016-05-05T11:41:25.000Z","pubdate":"2016-05-05T11:41:25.000Z","pubDate":"2016-05-05T11:41:25.000Z","link":"http://blog.yld.io/2016/05/05/container-camp-wrap-up/","guid":"389eb264-a9da-4d21-b7f4-16c0f327404f","author":"Luke Bond","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/05/cc-social--1-.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Container Camp San Francisco 2016: Wrap-Up"},"rss:description":{"@":{},"#":"<p>The fourth incarnation of Container Camp, and the second in San Francisco, took place on Friday April 15th. <a href=\"https://yld.io\">YLD</a> were in attendance and we've put together this follow-up blog post for those who couldn't be there.</p>\n\n<p>One of the great things about Container Camp has always been the broad representation</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/05/05/container-camp-wrap-up/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"389eb264-a9da-4d21-b7f4-16c0f327404f"},"dc:creator":{"@":{},"#":"Luke Bond"},"rss:pubdate":{"@":{},"#":"Thu, 05 May 2016 11:41:25 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/05/cc-social--1-.png","medium":"image"}},"content:encoded":{"@":{},"#":"The fourth incarnation of Container Camp, and the second in San Francisco, took place on Friday April 15th. YLD were in attendance and we've put together this follow-up blog post for those who couldn't be there.\n\nOne of the great things about Container Camp has always been the broad representation of the container ecosystem, rather than focusing on one container runtime or platform. This year was no exception, with talks about Docker, rkt, Kubernetes, Deis and Flynn, as well as covering important subjects like container security, cgroups and namespaces, unikernels and some production use-cases from Kaggle and Netflix.\n\nVideos for the talks are now being released and can be found here.\n\nThe Talks\n\nKeynote: The New Stack Research: Clarity About Container Orchestration for a Very Confused Market\n\nAlex Williams, Founder and Editor-in-Chief, The New Stack\n\nAlex presented some new research about container orchestration. The container ecosystem is still young and maturing, we are all still figuring out not only how to orchestrate containers in distributed applications, but also what terms like orchestration even mean in this new world. With new technologies come new skillsets and new, and sometimes redefined, terminology.\n\nKubernetes is the most used platform, followed by Mesos and then Docker Swarm. For service discovery Consul was the most popular, followed by ZooKeeper (used in Mesos) and Etcd. Traditional configuration management tools were not in common usage with containers, but Ansible figured highly in people's plans, suggesting that configuration management has not been forgotten but that the vendors are yet to fully integrate them.\n\nI was surprised by the popularity of Docker Swarm, which in my experience has felt immature. This shows how Docker are really nailing the developer experience, though.\n\nEverything we need to know about DevOps we learned from sci-fi movies\n\nCasey Bisson, Director of Product, Joyent\n\nCasey took us on an entertaining journey through sci-fi films that depict how we've viewed the future of computing in popular culture, positing that their depiction as being self-operating, self-healing and environment-independent should inform how we design cloud-native applications today. Casey proposes the \"autopilot\" pattern, discussed at Day Zero, as a way to achieve this vision.\n\nThis approach is contrary to the common practice of a centralised orchestrator doing this work, an approach I prefer but Casey's talk convinced me to at least take another look at this approach. I did, however, feel that his argument against a centralised orchestrator was a straw-man, and that good DevOps practices (i.e. not \"throwing code over the wall\" to ops) will address all of the issues he raised.\n\nWhat's new in Docker\n\nMano Marks, Director of Developer Relations, Docker\n\nMano gave us a brief outline of the new features in the latest Docker release: 1.11.\n\nThe biggest news is that this is the first version with an OCI-compatible runtime: it now uses containerd and runc. This was the big focus of 1.11, hence not otherwise a feature-rich release. To read more about this, check out my container ecosystem overview and also my recent talk about rkt.\n\nOther features of note include:\n\n\nDNS round-robin load balancing in Swarm\nSwarm rescheduling is no longer experimental\nDocker Machine Azure driver\n\n\nThe beta of Docker for Mac and Windows is also big news, and was the subject of many audience questions.\n\nContainer Standards and Interfaces: An Update\n\nBrandon Philips, CTO, CoreOS\n\nBrandon's talks are always great and this was no exception. He brought us up to date on the latest OCI news in relation to container standards.\n\nBrandon pulled no punches when contrasting CNI and CNM, promulgating the virtues of CNI's simple and interoperable design and wide adoption in the container ecosystem (but not by Docker).\n\nThis talk was the highlight of the day, in my opinion.\n\nCoreOS Fest Berlin is just around the corner, at which I'll be doing an introductory talk on rkt.\n\nWhat's New in Kubernetes\n\nTim Hockin, Kubernetes Technical Lead, Google\n\nAfter a quick intro to Kubernetes, Tim moved on to an overview of new features in 1.2, with demos, and then a peek into the future.\n\nNew features include:\n\n\nMulti-zone clusters with automatic balancing, no API changes, using labels. Works for GKE, GCE, AWS and can be done manually for on-premise installations\n\"Deployments\" are a new abstraction over replication controllers that will automatically nurse your new containers into production with zero-downtime and smooth rollback\n\"DaemonSets\" allow you to run something once and once only on all nodes (or a subset of nodes, using label-based selection)\n\"HorizontalPodAutoScalers\" are just what the name suggests- automatically scale pods as needed, based on CPU usage (for now; custom metrics are in alpha)\n\"Jobs\" are short-lived containers with result aggregation\n\"Secrets\" allow you to grant a pod access to a secured information such as secrets. The secrets are injected as virtual volumes that can be changed at runtime. Also supports environment variables\n\"ConfigMaps\" work like Secrets, but for injecting non-secret config\n\"Ingress\" - A new L7 HTTP load balancer with SSL\n\"PersistentVolumes\" abstracts storage from your app, and the volumes will live until they have no more users\n\n\nPlus a plethora of performance &amp; scalability improvements.\n\nKubernetes + PaaS: Bringing Containers to Production\n\nJason Hansen, Chief Architect, Deis\n\nJason makes a compelling case for PaaS as a pain-reducer and enabler of fast-paced change. He presented the latest incarnation of the Deis PaaS, now re-platformed onto Kubernetes.\n\nEarly versions of Deis worked by git pushing code to the PaaS and it would make Heroku buildpacks for your app. However, now it can pull images from Docker registries, which is much more useful for production workflows.\n\nAn Introduction to Container Security\n\nThomas Cameron, Global Solutions Architect Leader, Redhat\n\nThomas walked us through what a container is, explaining cgroups, namespaces and libcap, as well as various other security features of Linux (including SELinux), putting Linux container security in context.\n\nThomas also explored how having root in a container maps to the underlying kernel, and how these various tools can effectively block any access to kernel subsystems that are not needed, one example was giving a user root access, filtering their access and then demonstrating that this \"root\" user was unable to disable their filters.\n\nSecurity is pretty. Security is good. But it should never be pretty good\n\nCynthia Thomas, Systems Engineer, Midokura\n\nCynthia's talk was something of a call to arms to improve network security in the container world; it shouldn't be an afterthought.\n\nAs we move to containerised, microservice architectures for our applications we get increased network complexity due to running multiple applications on each host. This results in increased network policy complexity.\n\nNeutron, a virtual network service for OpenStack, can be used to secure Docker container networks via kuryr, a Docker libnetwork remote driver. It doesn't require a full OpenStack installation.\n\nContaining Databases\n\nJonathan Rudenberg, Founder &amp; CTO, Flynn\n\nAn early pioneer in the Docker PaaS space, Flynn is a project I've been watching for a few years. You can read more about Flynn at (https://flynn.io).\n\nJonathan talked about Flynn's database replication. Flynn uses a mixture of synchronous and asynchronous replication to provide highly available databases that survive network partitions.\n\nApplication Sandboxes vs. Containers\n\nJessica Frazelle, Security Engineer, Mesosphere\n\nJessie gave us an overview of what containers are at a low level, before discussing application sandboxes. Google Chrome is an example of an application sandbox; it uses container-like primitives to achieve some isolation between processes, but without running as root. Chrome uses user namespaces and seccomp but not cgroups.\n\nJessie gave us a demo of a pretty cool project she has been working on: a tool that generates an executable containing a container image that can run as a non-root user using runc.\n\nPretty cool stuff.\n\nUnikernels: Practical Advice for Juggling Chainsaws\n\nJohn Feminella, Technical Advisor Cloud Foundry, Pivotal\n\nJohn's talk was a entry-level introduction to unikernel concepts, benefits and challenges, as well as some of the low-level Linux isolation features underpinning containers.\n\nDocker and Reproducible Data Science\n\nBen Hamner, Co-Founder &amp; CTO, Kaggle\n\nKaggle have built a container-based platform for running reproducible scientific data analysis, and Ben discussed some of the unique features of containers that make it suitable for this task.\n\nSlaying Monoliths with Docker and Node.js, a Netflix Original\n\nYunong Xiao, Software Engineer, Netflix\n\nNetflix's Global expansion and rise in popularity means they have some unique scaling challenges. Yunong talked us through their journey from a multi-tiered, REST-based architecture to a self-service container-based system for Netflix client application developers, using Node.js.\n\nConclusion\n\nCongratulations to organisers Angie and Chris for a smooth operation and a great selection of talks. Container Camp continues to carve out its niche as an independent, fun and deeply technical event for developers that caters for beginners and experts alike."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Encrypting Communication","description":"In the previous post of this series we saw how we could perform RPC on top of a multiplexed duplex stream. In this article we're going to show you how you increase the security of","summary":"<p>In <a href=\"http://blog.yld.io/2016/03/23/multiplexing-streams\">the previous post of this series</a> we saw how we could perform RPC on top of a multiplexed duplex stream. In this article we're going to show you how you increase the security of the client and the server applications by encrypting your communication channel and adding authentication.</p>\n\n<p>Enjoy!</p>","date":"2016-04-21T15:19:01.000Z","pubdate":"2016-04-21T15:19:01.000Z","pubDate":"2016-04-21T15:19:01.000Z","link":"http://blog.yld.io/2016/04/21/encrypting-communication/","guid":"39ddae93-f492-48e8-9fc2-727a28a8ef98","author":"Pedro Teixeira","comments":null,"origlink":null,"image":{},"source":{},"categories":["node.js","networking","patterns"],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-1.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Encrypting Communication"},"rss:description":{"@":{},"#":"<p>In <a href=\"http://blog.yld.io/2016/03/23/multiplexing-streams\">the previous post of this series</a> we saw how we could perform RPC on top of a multiplexed duplex stream. In this article we're going to show you how you increase the security of the client and the server applications by encrypting your communication channel and adding authentication.</p>\n\n<p>Enjoy!</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/04/21/encrypting-communication/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"39ddae93-f492-48e8-9fc2-727a28a8ef98"},"rss:category":[{"@":{},"#":"node.js"},{"@":{},"#":"networking"},{"@":{},"#":"patterns"}],"dc:creator":{"@":{},"#":"Pedro Teixeira"},"rss:pubdate":{"@":{},"#":"Thu, 21 Apr 2016 15:19:01 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-1.png","medium":"image"}},"content:encoded":{"@":{},"#":"In the previous post of this series we saw how we could perform RPC on top of a multiplexed duplex stream. In this article we're going to show you how you increase the security of the client and the server applications by encrypting your communication channel and adding authentication.\n\nEnjoy!\n\n\n\nSo far our TCP server has communicated with clients without any kind of security: it lets any client connect to it and it communicates with them through a clear-text channel, making it prone to someone eavesdropping on the network. These may not be a big problem for your office refrigerator application, but it can surely be if your data is sensitive, or you can't let just every and anyone use the service.\n\nLet's first address this last problem: encrypting the communication channel.\n\nCreating self-signed certificates\n\nIn this example we will create a self-signed certificate. This is a certificate that was not issued by a recognised Certificate Authority (CA), which means that the clients won't be able to verify this certificate unless they have access to the public certificate we're about to issue.\n\nIf you can distribute the server public certificate to the clients, this is the most flexible approach where you don't have to rely on a third party to issue your certificate.\n\nBefore you can generate a certificate you need to have the openssl command-line tool installed. Many Linux distributions, and MacOS, come with the openssl program installed.\n\n\n  If you're running Windows, you can try downloading the windows binary distribution from here:\n  http://www.openssl.org/related/binaries.html.\n\n\nCreating a Certification Authority\n\nNow you're going to create your own Certification Authority, which you will later use to create the server and client certificates.\n\nOnce you have that installed, create a certs/ca directory, and in it, generate the CA private key:\n\n$ mkdir -p certs/ca\n$ cd certs/ca\n$ openssl genrsa -out private-key.pem 2048\n\n\nThis creates a suitable private key and writes it to the private-key.pem file.\n\nNext we're going to create the CA public certificate based on the private key:\n\n$ openssl req -x509 -new -nodes -key private-key.pem -days 1024 -out certificate.pem -subj \"/C=US/ST=Utah/L=Provo/O=ACME Signing Authority Inc/CN=example.com\"\n\n\nThis outputs the CA certificate into the certificate.pem file.\n\nCreating the server key and certificate\n\nNow we need to issue the server certificate, but first let's create the private keys inside the certs/server directory:\n\n$ cd ..\n$ mkdir server\n$ cd server\n$ openssl genrsa -out private-key.pem 2048\n\n\nNext, create a Certificate Signing Request (CSR) file using your private key:\n\n$ openssl req -new -key private-key.pem -out csr.pem\n\n\nThe purpose of this CSR is to \"request\" a certificate. That is, if you wanted a CA to sign your certificate, you could give them this file to process and they would give you back a certificate.\n\nHowever, here you will self-sign your certificate, again using your CA private key:\n\n$ openssl x509 -req -in csr.pem -CA ../root/certificate.pem -CAkey ../root/private-key.pem -CAcreateserial -out certificate.pem -days 500\n\n\nAdding TLS support to the server\n\nNow we need to modify the server to use TLS instead of raw TCP:\n\nfridge_server.js:\n\nvar fs = require('fs');  \nvar tls = require('tls');  \nvar path = require('path');  \nvar DuplexEmitter = require('duplex-emitter');  \nvar Mux = require('mux-demux');  \nvar dnode = require('dnode');  \nvar commands = require('./commands');\n\nvar serverOptions = {  \n  key:  fs.readFileSync(path.join(__dirname, 'certs', 'server', 'private-key.pem')),\n  cert: fs.readFileSync(path.join(__dirname, 'certs', 'server', 'certificate.pem'))\n};\n\nvar server = tls.createServer(serverOptions);\n\nserver.on('secureConnection', handleConnection);\n\nserver.listen(8000, function() {  \n  console.log('door server listening on %j', server.address());\n});\n\n\n// sensors\n\nvar sensors = [  \n  {\n    name: 'door',\n    events: ['open', 'close'],\n    emitter: require('./door'),\n    remotes: {},\n    nextId: 0,\n    lastEvent: undefined\n  },\n  {\n    name: 'temperature',\n    events: ['reading'],\n    emitter: require('./thermometer'),\n    remotes: {},\n    nextId: 0,\n    lastEvent: undefined\n  },\n];\n\n\n// handle connections\n\nfunction handleConnection(conn) {  \n  var mx = Mux(handleConnection);\n\n  conn.on('error', onError);\n  mx.on('error', onError);\n\n  conn.pipe(mx).pipe(conn);\n\n  sensors.forEach(attachSensor);\n\n  function attachSensor(sensor) {\n    var stream = mx.createWriteStream(sensor.name);\n    var remoteEmitter = DuplexEmitter(stream);\n\n    stream.once('close', onClose);\n    stream.on('error', onError);\n    mx.on('error', onError);\n\n    // add remote to sensor remotes\n    var id = ++ sensor.nextId;\n    sensor.remotes[id] = remoteEmitter;\n\n    if (sensor.lastEvent) {\n      remoteEmitter.emit.apply(remoteEmitter, sensor.lastEvent);\n    }\n\n    function onClose() {\n      delete sensor.remotes[id];\n    }\n\n  }\n\n\n  /// RPC\n\n  function handleConnection(conn) {\n    if (conn.meta != 'rpc') {\n      onError(new Error('Invalid stream name: ' + conn.meta));\n    }\n    else {\n      var d = dnode(commands);\n      conn.pipe(d).pipe(conn);\n    }\n  }\n\n\n  function onError(err) {\n    conn.destroy();\n    console.error('Error on connection: ' + err.message);\n  }\n\n}\n\n\n/// broadcast all sensor events to connections\n\nsensors.forEach(function(sensor) {  \n  sensor.events.forEach(function(event) {\n\n    // broadcast all events of type `event`\n    sensor.emitter.on(event, broadcast(event, sensor.remotes));\n\n    // store last event on `sensor.lastEvent`\n    sensor.emitter.on(event, function() {\n      var args = Array.prototype.slice.call(arguments);\n      args.unshift(event);\n      sensor.lastEvent = args;\n    });\n  });\n});\n\nfunction broadcast(event, remotes) {  \n  return function() {\n    var args = Array.prototype.slice.call(arguments);\n    args.unshift(event);\n\n    Object.keys(remotes).forEach(function(emitterId) {\n      var remote = remotes[emitterId];\n      remote.emit.apply(remote, args);\n    });\n\n  };\n}\n\n\nHere you can see that we're using tls.createServer(options) to create our server, passing in some options:\n\n\nkey: the server private key, loaded as a raw buffer\ncert: the server certificate, also loaded as a raw buffer\n\n\nWhen a client successfully establishes a secure connection, the server emits a secureConnection event, which is what we want to listen to now.\n\n\n  The server also emits a connect event, but that gives you a unencrypted TCP socket as before, which is not what you need now.\n\n\nUsing TLS on the client\n\nNow, let's enable TLS also on the client:\n\nvar fs = require('fs');  \nvar tls = require('tls');  \nvar path = require('path');  \nvar Mux = require('mux-demux');  \nvar DuplexEmitter = require('duplex-emitter');\n\n\nvar options = {  \n  host: process.argv[2],\n  port: Number(process.argv[3]),\n  rejectUnauthorized: false\n};\n\nvar conn = tls.connect(options, onConnect);\n\nconn.on('error', function(err) {  \n  console.log('error: %j', err);\n});\n\nvar doorTimeoutSecs = Number(process.argv[4]);  \nvar maxTemperature = Number(process.argv[5]);\n\n\nvar sensors = {  \n  'door': handleDoor,\n  'temperature': handleTemperature\n};\n\nfunction onConnect() {  \n  console.log('connected', conn.authorized, conn.authorizationError);\n\n  var mx = Mux(onStream);\n  conn.pipe(mx).pipe(conn);\n\n  function onStream(stream) {\n    var handle = sensors[stream.meta];\n    if (! handle) {\n      throw new Error('Unknown stream: %j', stream.meta);\n    }\n    handle(DuplexEmitter(stream));\n  }\n}\n\n/// Door\n\nfunction handleDoor(door) {  \n  var timeout;\n  var warned = false;\n\n  door.on('open', onDoorOpen);\n  door.on('close', onDoorClose);\n\n  function onDoorOpen() {\n    timeout = setTimeout(onDoorTimeout, doorTimeoutSecs * 1e3);\n  }\n\n  function onDoorClose() {\n    if (warned) {\n      warned = false;\n      console.log('closed now');\n    }\n    if (timeout) {\n      clearTimeout(timeout);\n    }\n  }\n\n  function onDoorTimeout() {\n    warned = true;\n    console.error(\n      'DOOR OPEN FOR MORE THAN %d SECONDS, GO CLOSE IT!!!',\n      doorTimeoutSecs);\n  }\n}\n\n\n/// Temperature\n\nfunction handleTemperature(temperature) {  \n  temperature.on('reading', onTemperatureReading);\n\n  function onTemperatureReading(temp, units) {\n    if (temp &gt; maxTemperature) {\n      console.error('FRIDGE IS TOO HOT: %d %s', temp, units);\n    }\n  }\n}\n\n\nWe're now using tls.createConnection(options) to connect to the server, passing in some options:\n\n\nhost: hostname from the command-line arguments, the same as before;\nport: TCP port number from the command-line arguments, same as before;\nrejectUnauthorized: false, telling the client that it should not check the server certificate validity. This option is here because, for now, we just wish to have an encrypted channel – we're not looking into authenticating the server (yet).\n\n\nAuthenticating the server\n\nLet's now allow the client to authenticate the server by making just some small changes to the tls.connect options:\n\n// ...\nvar options = {  \n  host: process.argv[2],\n  port: Number(process.argv[3]),\n  ca: [fs.readFileSync(path.join(__dirname, 'certs', 'root', 'certificate.pem'))],\n  rejectUnauthorized: true\n};\n// ...\n\n\nHere we've added one attribute to the config: ca, where we define the root Certification Authorities that the client will take into account when validating the server certificate. Here we pass in the root public certificate, admitting that the client trusts the certificates signed by it.\n\nAlso, we change rejectUnauthorized from false to true. If, when establishing a secure connection, the client cannot, for some reason, verify that the server certificate was issued by any of the trusted Certification Authorities (or any of their child CAs) or that the server name does not match the common name field (CN) in the certificate data, the connection emits an error and closes.\n\nIf, by chance, you choose to run this server in a different host name, you will have to reissue the certificate, changing its CN field.\n\nYou can test this by launching the server on a command-line window:\n\n$ node fridge_server\ndoor server listening on {\"address\":\"0.0.0.0\",\"family\":\"IPv4\",\"port\":8000}  \n\n\nand then launching the client on another:\n\n$ node fridge_client localhost 8000 1 10\n\n\nAuthenticating the client\n\nFor the server to authenticate the client we need to use our CA to issue a new certificate. First though, we need to create a client certificate:\n\n$ mkdir certs/client-001\n$ cd certs/client-001\n$ openssl genrsa -out private-key.pem 2048\n\n\nNext, create a Certificate Signing Request (CSR) file using your private key, just like you did when you requested the server one:\n\n$ openssl req -new -key private-key.pem -out csr.pem\n\n\nLet's then use our CA to create the signed certificate:\n\n$ openssl x509 -req -in csr.pem -CA ../root/certificate.pem -CAkey ../root/private-key.pem -CAcreateserial -out certificate.pem -days 500\n\n\nNow we can use it on the client, modifying the client options to add the private key and the public certificate:\n\n// ...\nvar options = {  \n  host: process.argv[2],\n  port: Number(process.argv[3]),\n  ca: [fs.readFileSync(path.join(__dirname, 'certs', 'root', 'certificate.pem'))],\n  key: fs.readFileSync(path.join(__dirname, 'certs', 'client-001', 'private-key.pem')),\n  cert: fs.readFileSync(path.join(__dirname, 'certs', 'client-001', 'certificate.pem')),\n  rejectUnauthorized: true\n};\n\n\nOn the server side, let's change some options to force the server to get the client certificate and validate the certificate against our CA:\n\n//...\nvar serverOptions = {  \n  key:  fs.readFileSync(path.join(__dirname, 'certs', 'server', 'private-key.pem')),\n  cert: fs.readFileSync(path.join(__dirname, 'certs', 'server', 'certificate.pem')),\n  ca: [fs.readFileSync(path.join(__dirname, 'certs', 'root', 'certificate.pem'))],\n  requestCert: true,\n  rejectUnauthorized: true\n};\n\n\nRevoking access\n\nOnce one client certificate is issued, our server will accept it forever. To be able to reject any given client with a valid certificate, we'd have to keep a list of supported client names (white list) or a list of banned client names (black list). To make things simpler for now, let's keep a white list in the server memory, against which we check the client certificate before accepting a connection.\n\nOn the server, we can then simply create this client white list, containing only our one client name for now:\n\n/// client white list\nvar clientWhiteList = [  \n  'fridge-client-001'\n];\n\nfunction clientAllowed(name) {  \n  return clientWhiteList.indexOf(name) &gt; -1;\n}\n\n\nNow, in the connection handler we can fetch the client certificate and then use the clientAllowed function to check whether we can proceed:\n\n//...\n\nfunction handleConnection(conn) {\n\n  var mx = Mux(handleConnection);\n\n  conn.on('error', onError);\n  mx.on('error', onError);\n\n  var clientName = conn.getPeerCertificate().subject.CN;\n  if (! clientAllowed(clientName)) {\n    return conn.emit('error', new Error('client not allowed: ' + clientName));\n  }\n\n  conn.pipe(mx).pipe(conn);\n\n  //...\n\n\nThis method of whitelisting client names requires that you restart your process every time there is a change. You can implement a gossip protocol over the network where all the changes (inserts or removals, in this case) in a given list are automatically propagated to all of the nodes within a certain amount of time. For more information you can check another book in this series named \"Configuration Patterns\".\n\n\n\nNext Article\n\nIn our next article for this series we'll start taking a look at queues and how you can use them streamline and share asynchronous work between processes.\n\n\n\nThis article was extracted from the Networking Patterns, a book from the Node Patterns series."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"My internship experience at YLD","description":"I got in touch with YLD for a 6 month marketing internship position and was interviewed in September 2015. After an initial Skype call, I was invited to the YLD office for an one-to-one interview","summary":"<p>I got in touch with YLD for a 6 month marketing internship position and was interviewed in September 2015. After an initial Skype call, I was invited to the YLD office for an one-to-one interview with Head of Marketing, Laura Plaga. A week afterwards, I was offered a position and</p>","date":"2016-04-19T17:03:31.000Z","pubdate":"2016-04-19T17:03:31.000Z","pubDate":"2016-04-19T17:03:31.000Z","link":"http://blog.yld.io/2016/04/19/my-internship-experience-at-yld/","guid":"ba1be3f3-c2db-4584-aa1a-26f4578d8ff4","author":"Olivia Kingsbury","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"My internship experience at YLD"},"rss:description":{"@":{},"#":"<p>I got in touch with YLD for a 6 month marketing internship position and was interviewed in September 2015. After an initial Skype call, I was invited to the YLD office for an one-to-one interview with Head of Marketing, Laura Plaga. A week afterwards, I was offered a position and</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/04/19/my-internship-experience-at-yld/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"ba1be3f3-c2db-4584-aa1a-26f4578d8ff4"},"dc:creator":{"@":{},"#":"Olivia Kingsbury"},"rss:pubdate":{"@":{},"#":"Tue, 19 Apr 2016 17:03:31 GMT"},"content:encoded":{"@":{},"#":"I got in touch with YLD for a 6 month marketing internship position and was interviewed in September 2015. After an initial Skype call, I was invited to the YLD office for an one-to-one interview with Head of Marketing, Laura Plaga. A week afterwards, I was offered a position and I was ecstatic! \n\nMy role started off open-ended, primarily relating to Marketing and running the social platforms. As time went on, I primarily devoted of my efforts to event coordination, content writing, and editing company documents to make things go smoothly. \n\nThe first week was a learning curve, as most first weeks are at a new job. Originally I was intimidated to join because of the team’s rapid growth as a company and numerous achievements. I (incorrectly) assumed I would be talked-down-to or patronised, as I do not come from a large technology background. Despite this, I stood corrected as the YLD team was inclusive, fun, and accommodating to a student like myself. One of the things that stood out most to me was YLD’s extremely cooperative attitude towards my university schedule. The whole team emanated a collaborative and hard-working spirit that was quickly adopted by every team member. \n\nDuring my time at YLD, some of my favourite projects I have worked on include helping to plan the company’s second birthday party at the Skyloft in November of 2015, and assisting in the rebranding process of early 2016. Additionally, I worked alongside Laura to construct a plan for every piece of content YLD plans to publish. This allows YLD to deliver consistent, original, and engaging posts on a weekly basis.  \n\nTo sum up my work experience in a few words I would use: dynamic, challenging, and exciting. The entire YLD team is friendly, hilarious, and most importantly willing to share their respected advice with those who want to learn. Another enriching trait about YLD is that every individual brings a unique background and skillset to the table, creating a mixing pot of different cultures and insight. All in all, it was a pleasure to work alongside of these great minds for the past six months. I can safely say I have taken away  invaluable transferrable skills, memories I will recall fondly, and the ability to call myself #iAmYLD! "},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Container Camp Day Zero","description":"On the evening before Container Camp we attended the Joyent-sponsored \"Day Zero\" event at New Relic - an evening of demo-focused presentations around containers.\n\nMasterfully emceed by New Relic's Jacob Groundwater and boasting speakers from Mesosphere,","summary":"<p>On the evening before Container Camp we attended the Joyent-sponsored \"Day Zero\" event at New Relic - an evening of demo-focused presentations around containers.</p>\n\n<p>Masterfully emceed by New Relic's Jacob Groundwater and boasting speakers from Mesosphere, Hashicorp and Joyent, I knew we'd be in for a treat.</p>\n\n<h2 id=\"buildingcontainersinpurebashandc\">Building Containers in</h2>","date":"2016-04-18T09:20:10.000Z","pubdate":"2016-04-18T09:20:10.000Z","pubDate":"2016-04-18T09:20:10.000Z","link":"http://blog.yld.io/2016/04/18/container-camp-day-zero/","guid":"572bffb5-eea7-4b2c-9f45-9099cce6c652","author":"Luke Bond","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/04/cc-social--1--3.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Container Camp Day Zero"},"rss:description":{"@":{},"#":"<p>On the evening before Container Camp we attended the Joyent-sponsored \"Day Zero\" event at New Relic - an evening of demo-focused presentations around containers.</p>\n\n<p>Masterfully emceed by New Relic's Jacob Groundwater and boasting speakers from Mesosphere, Hashicorp and Joyent, I knew we'd be in for a treat.</p>\n\n<h2 id=\"buildingcontainersinpurebashandc\">Building Containers in</h2>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/04/18/container-camp-day-zero/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"572bffb5-eea7-4b2c-9f45-9099cce6c652"},"dc:creator":{"@":{},"#":"Luke Bond"},"rss:pubdate":{"@":{},"#":"Mon, 18 Apr 2016 09:20:10 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/04/cc-social--1--3.png","medium":"image"}},"content:encoded":{"@":{},"#":"On the evening before Container Camp we attended the Joyent-sponsored \"Day Zero\" event at New Relic - an evening of demo-focused presentations around containers.\n\nMasterfully emceed by New Relic's Jacob Groundwater and boasting speakers from Mesosphere, Hashicorp and Joyent, I knew we'd be in for a treat.\n\nBuilding Containers in Pure Bash and C\n\nJessica Frazelle, Security Engineer, Mesosphere\n\nJessie took us through examples of creating namespaced processes (more or less a container) using C (with the clone syscall) and also from the command-line using bash (with unshare(2)).\n\n\n  If you take anything away from this it should be an appreciation for unshare(2).\n\n\n-- Jessica Frazelle\n\nAutomating Infrastructure with Packer and Terraform\n\nJames Nugent, Engineering, Hashicorp\n\nJames gave us a whirlwind, yet thorough, overview of how Packer and Terraform can be used to package, provision and deploy containers to the cloud.\n\nWe learned that Packer can create Docker images for you, with its own configuration file syntax that you can use instead of a Dockerfile. The crowd seemed to agree that Dockerfiles are not the most pleasant format to work with.\n\nApplications on Autopilot\n\nTim Gross, Product Manager, Joyent\n\nTim walked us through the Autopilot Pattern, embodied in Container Autopilot (formerly called Container Buddy), a PID1 wrapper (init process) to run inside your containers.\n\nThe Autopilot pattern says that containers should manage their own startup, shutdown, service discovery and load balancing, which is taken care of for you by Container Autopilot. An interesting concept, one which we haven't had success with in the past but Container Autopilot does appear to be a mature and useful tool.\n\nStay tuned for our Container Camp review!\n\nWe will be releasing a wrap-up blog post after the event for those who can't be there.\n\nAlso check out our pre-ContainerCamp blog post on the container ecosystem for a refresher before the big day.\n\nYLD are a platinum sponsor of Container Camp and have donated 8 tickets for academic and diversity applicants for Container Camp."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Turning a stream into an RPC channel","description":"Further developing our refrigerator project, you may want to add not only new sensors, but also some new functions like changing the fridge target temperature, starting a defrost cycle, or even turning it on or","summary":"<p>Further developing <a href=\"http://blog.yld.io/2016/03/23/multiplexing-streams\">our refrigerator project</a>, you may want to add not only new sensors, but also some new functions like changing the fridge target temperature, starting a defrost cycle, or even turning it on or off. We could expose some of these functions as events to be sent by the</p>","date":"2016-04-07T12:46:37.000Z","pubdate":"2016-04-07T12:46:37.000Z","pubDate":"2016-04-07T12:46:37.000Z","link":"http://blog.yld.io/2016/04/07/remote-procedure-call/","guid":"7eb987b4-2283-425e-9591-a55be1c73c33","author":"Pedro Teixeira","comments":null,"origlink":null,"image":{},"source":{},"categories":["node.js","networking","patterns"],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-2.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Turning a stream into an RPC channel"},"rss:description":{"@":{},"#":"<p>Further developing <a href=\"http://blog.yld.io/2016/03/23/multiplexing-streams\">our refrigerator project</a>, you may want to add not only new sensors, but also some new functions like changing the fridge target temperature, starting a defrost cycle, or even turning it on or off. We could expose some of these functions as events to be sent by the</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/04/07/remote-procedure-call/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"7eb987b4-2283-425e-9591-a55be1c73c33"},"rss:category":[{"@":{},"#":"node.js"},{"@":{},"#":"networking"},{"@":{},"#":"patterns"}],"dc:creator":{"@":{},"#":"Pedro Teixeira"},"rss:pubdate":{"@":{},"#":"Thu, 07 Apr 2016 12:46:37 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-2.png","medium":"image"}},"content:encoded":{"@":{},"#":"Further developing our refrigerator project, you may want to add not only new sensors, but also some new functions like changing the fridge target temperature, starting a defrost cycle, or even turning it on or off. We could expose some of these functions as events to be sent by the client into the server, but events are not really a good suit for this type of remote access function. Instead, we should use another abstraction: a remote procedure call.\n\nThe server\n\nLet's then expose an RPC channel on our refrigerator server, using a Mux-Demux sub-stream and the dnode module. Let's first instal dnode:\n\n$ npm install dnode\n\n\nWe can then add some commands to our fake refrigerator server, encapsulated in the commands.js local module:\n\ncommands.js:\n\nvar commands = exports;\n\ncommands.setTargetTemperature = randomTimeout(null, 'command accepted, target temperature set');  \ncommands.defrost = randomTimeout(null, 'command accepted, defrosting');  \ncommands.powerOn = randomTimeout(null, 'command accepted, fridge is on now');  \ncommands.powerOff = randomTimeout(null, 'command accepted, fridge is off now');\n\nfunction randomTimeout() {  \n  var replyArgs = Array.prototype.slice.call(arguments);\n\n  return function() {\n    var callback = arguments[arguments.length - 1];\n    if (typeof callback == 'function') {\n      var timeout = Math.floor(Math.random() * 1000);\n      var args = [callback, timeout].concat(replyArgs);\n      setTimeout.apply(null, args);\n    }\n  }\n}\n\n\nThis module exposes a set of commands (setTargetTemperature, defrost, powerOn and powerOff) as functions. Here we're taking a shortcut to easily define a function that calls back with the given arguments somewhere before one second has elapsed.\n\nNow we need to change our refrigerator server to expose these methods remotely:\n\nfridge_server.js:\n\nvar net = require('net');  \nvar DuplexEmitter = require('duplex-emitter');  \nvar Mux = require('mux-demux');  \nvar dnode = require('dnode');  \nvar commands = require('./commands');\n\nvar server = net.createServer();\n\nserver.on('connection', handleConnection);\n\nserver.listen(8000, function() {  \n  console.log('door server listening on %j', server.address());\n});\n\n\n// sensors\n\nvar sensors = [  \n  {\n    name: 'door',\n    events: ['open', 'close'],\n    emitter: require('./door'),\n    remotes: {},\n    nextId: 0,\n    lastEvent: undefined\n  },\n  {\n    name: 'temperature',\n    events: ['reading'],\n    emitter: require('./thermometer'),\n    remotes: {},\n    nextId: 0,\n    lastEvent: undefined\n  },\n];\n\n\n// handle connections\n\nfunction handleConnection(conn) {  \n  var mx = Mux(handleConnection);\n\n  conn.on('error', onError);\n  mx.on('error', onError);\n\n  conn.pipe(mx).pipe(conn);\n\n  sensors.forEach(attachSensor);\n\n  function attachSensor(sensor) {\n    var stream = mx.createWriteStream(sensor.name);\n    var remoteEmitter = DuplexEmitter(stream);\n\n    stream.once('close', onClose);\n    stream.on('error', onError);\n    mx.on('error', onError);\n\n    // add remote to sensor remotes\n    var id = ++ sensor.nextId;\n    sensor.remotes[id] = remoteEmitter;\n\n    if (sensor.lastEvent) {\n      remoteEmitter.emit.apply(remoteEmitter, sensor.lastEvent);\n    }\n\n    function onClose() {\n      delete sensor.remotes[id];\n    }\n\n  }\n\n\n  /// RPC\n\n  function handleConnection(conn) {\n    if (conn.meta != 'rpc') {\n      onError(new Error('Invalid stream name: ' + conn.meta));\n    }\n    else {\n      var d = dnode(commands);\n      conn.pipe(d).pipe(conn);\n    }\n  }\n\n\n  function onError(err) {\n    conn.destroy();\n    console.error('Error on connection: ' + err.message);\n  }\n\n}\n\n\n/// broadcast all sensor events to connections\n\nsensors.forEach(function(sensor) {  \n  sensor.events.forEach(function(event) {\n\n    // broadcast all events of type `event`\n    sensor.emitter.on(event, broadcast(event, sensor.remotes));\n\n    // store last event on `sensor.lastEvent`\n    sensor.emitter.on(event, function() {\n      var args = Array.prototype.slice.call(arguments);\n      args.unshift(event);\n      sensor.lastEvent = args;\n    });\n  });\n});\n\nfunction broadcast(event, remotes) {  \n  return function() {\n    var args = Array.prototype.slice.call(arguments);\n    args.unshift(event);\n\n    Object.keys(remotes).forEach(function(emitterId) {\n      var remote = remotes[emitterId];\n      remote.emit.apply(remote, args);\n    });\n\n  };\n}\n\n\nBesides all the sensor set-up we already had in the previous version of our server, now we're listening for new Mux-Demux streams by passing a function to the Mux-Demux constructor:\n\nvar mx = Mux(handleConnection);\n\n/// ...\n\nfunction handleConnection(conn) {  \n    if (conn.meta != 'rpc') {\n      onError(new Error('Invalid stream name: ' + conn.meta));\n    }\n    else {\n      var d = dnode(commands);\n      conn.pipe(d).pipe(conn);\n    }\n  }\n\n\nThis connection handler handles the RPC stream by creating a dnode endpoint with the given commands. This dnode endpoint is a stream, which we attach to and from our Mux-Demux stream:\n\nconn.pipe(d).pipe(conn);  \n\n\nThe client\n\nThose should be all the changes we need to perform on the server. Now we need to create a new client that accepts command-line arguments and dynamically invokes our RPC service.\n\nsend_command.js:\n\n#!/usr/bin/env node\n\nvar net = require('net');  \nvar Mux = require('mux-demux');  \nvar dnode = require('dnode');\n\nvar hostname = process.argv[2];  \nvar port = Number(process.argv[3]);  \nvar args = process.argv.slice(4);  \nvar method = args.shift();  \nif (! method) throw new Error('please provide a method to call');\n\nconsole.log('command: %s (%j)', method, args.join(', '));\n\nvar conn = net.connect(port, hostname);\n\nvar mx = Mux(onConnection);  \nconn.pipe(mx).pipe(conn);\n\nvar stream = mx.createStream('rpc');\n\nvar d = dnode();  \nstream.pipe(d).pipe(stream);\n\nd.on('remote', onRemote);\n\n\nfunction onRemote(remote) {  \n  // call the method\n  args.push(callback);\n  var fn = remote[method];\n  if (! fn) throw new Error('No such method: ' + method);\n  fn.apply(remote, args);\n}\n\n\nfunction callback(err, result) {  \n  if (err) throw err;\n  console.log('result: %j', result);\n  conn.end();\n}\n\nfunction onConnection(conn) {  \n}\n\n\nBesides command-line argument parsing and the usual connection set-up, we now create a Mux-Demux stream named rpc:\n\nvar stream = mx.createStream('rpc');  \n\n\nThis stream is a duplex one: both readable and writable — the client writes the RPC calls and reads the results. Now we can create a dnode endpoint and pipe it from and to this stream:\n\nvar d = dnode();  \nstream.pipe(d).pipe(stream);  \n\n\nHaving the new version of the server up and running, we can now use this send_command client to send commands:\n\n$ node send_command localhost 8000 setTargetTemperature 10\ncommand: setTargetTemperature (\"10\")  \nresult: \"command accepted, target temperature set\"  \n\n\nYou can now try sending it any of the other valid commands, defrost, powerOn or powerOff.\n\nNext article: Encrypting communication\n\nSo far our TCP server has communicated with clients without any kind of security: it lets any client connect to it and it communicates with them through a clear-text channel, making it prone to someone eavesdropping on the network. These may not be big problems for your office refrigerator application, but it can surely be if your data is sensitive, or you can't let just every and any one use the service.\n\n\n\nThis article was extracted from the Networking Patterns, a book from the Node Patterns series."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"The Linux Container Ecosystem - A pre Container Camp Primer","description":"Container Camp number 4 is just around the corner on the 15th of April 2016 at Bespoke in downtown San Francisco. As proud supporters of Container Camp, keen Linux Container enthusiasts, and early adopters, we","summary":"<p><a href=\"https://container.camp\">Container Camp number 4</a> is just around the corner on the 15th of April 2016 at Bespoke in downtown San Francisco. As proud supporters of Container Camp, keen Linux Container enthusiasts, and early adopters, we thought we'd take the opportunity to put together a Linux Container ecosystem overview as your</p>","date":"2016-04-05T12:52:45.000Z","pubdate":"2016-04-05T12:52:45.000Z","pubDate":"2016-04-05T12:52:45.000Z","link":"http://blog.yld.io/2016/04/05/the-linux-container-ecosystem-a-pre-container-camp-primer/","guid":"14f81da6-fa00-410a-a16e-19aa707a4dfd","author":"Luke Bond","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/04/cc-social--1--2.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"The Linux Container Ecosystem - A pre Container Camp Primer"},"rss:description":{"@":{},"#":"<p><a href=\"https://container.camp\">Container Camp number 4</a> is just around the corner on the 15th of April 2016 at Bespoke in downtown San Francisco. As proud supporters of Container Camp, keen Linux Container enthusiasts, and early adopters, we thought we'd take the opportunity to put together a Linux Container ecosystem overview as your</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/04/05/the-linux-container-ecosystem-a-pre-container-camp-primer/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"14f81da6-fa00-410a-a16e-19aa707a4dfd"},"dc:creator":{"@":{},"#":"Luke Bond"},"rss:pubdate":{"@":{},"#":"Tue, 05 Apr 2016 12:52:45 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/04/cc-social--1--2.png","medium":"image"}},"content:encoded":{"@":{},"#":"Container Camp number 4 is just around the corner on the 15th of April 2016 at Bespoke in downtown San Francisco. As proud supporters of Container Camp, keen Linux Container enthusiasts, and early adopters, we thought we'd take the opportunity to put together a Linux Container ecosystem overview as your pre-Container Camp refresher.\n\nWhat are Containers?\n\nLinux containers (LXC) are essentially isolated Linux processes, made possible by various new Linux kernel features over the last ten years or so (namely \"cgroups\"). The isolation that cgroups brings allows processes to be allocated a limited amount of CPU and memory (among other resources), and to be unable to access other parts of the filesystem (it is essentially a development of the \"chroot\").\n\nRaw LXC that is built in to Linux has been notoriously hard to use, and the explosion in excitement and adoption of Linux containers today is largely attributable to Docker's fantastic user experience for developers. By also addressing container packaging, distribution and discovery, Docker brought LXC to the mainstream. LXC existed before Docker, and technologies similar to LXC were available in FreeBSD and Solaris before Linux. But little of this mattered to most people until Docker came along.\n\nA Brief History of Containers\n\nAt Container Camp London 2015, Bryan Cantrill took us on a historical tour of how we got to where we are today: containers and microservices. Since VMs became ubiquitous, modern cloud computing has meant VMs running with hardware-level virtualisation. Bryan argues that this is too low in the stack, PaaS (platform-level virtualisation/abstraction) is too high, but that containers (OS-level virtualisation) are just right. In his 2015 San Francisco keynote he continued the story, bemoaning the fact that the state of the art of deploying Docker to production is to run containers in a VM, as it negates the economic advantage we gain from containers. It will be interesting to see any advances on these current practices in the Container Camp talks.\n\nWhilst the debate over systemd's very existence seems to have died down in the past year, there has been another interesting systemd-related story taking shape: Docker doesn't play well with systemd. Red Hat's Dan Walsh's has spoken about his frustrations trying to get good Docker integration into RedHat's systemd-based Linux distribution. The trouble is that both Docker and systemd want to orchestrate containers. Red Hat already ship a custom set of patches with Docker, will they fork Docker in order to integrate nicely with systemd? Will Docker relent? This is part of a continued shake-up of Linux's role; we're running Linux OSs as containers under Docker on Linux VMs on hypervisors, which is kind-of crazy. Some of those containers run systemd inside; many of the VMs run a systemd-based Linux distribution. Will we see more container-focused operating systems with the Docker daemon running as PID1, like RancherOS does? Will unikernels catch on and save us from this? I'm hoping we'll hear a few people talking about this at Container Camp.\n\nWhy Use Containers?\n\nThe business case for containers is too-often missing from the discussion. Whilst containers are primarily about process and resource isolation, this is not the most important value they can offer to your business. What containers enable has a lot more to do with breaking down the developer / operations divide (the DevOps philosophy). By providing a great user experience for developers around the packaging and sharing of container images, Docker showed us the value of shifting the ownership of OS dependencies from operations to the developer. This has a number of advantages:\n\n\nDevelopers are better placed than operations to maintain dependencies for the software that they write\nWhilst operations will always need to maintain approved Linux distributions and dependencies for some things, they are largely unburdened from this for new software\nShipping an application with its dependencies saves pain in production, such as missing or incorrect versions of shared libraries\nHuge improvements in dev-test feedback loop\n\n\nAs a result we can move faster, getting features in our customers' hands sooner. Docker helps to unlock the kind of pace at which modern software companies need to move in order to be competitive.\n\nStandards in the Container Ecosystem\n\nThe main standards bodies around containers at the moment are Appc, the Open Container Initiative (OCI) and the Cloud Native Computing Foundation (CCNF). Appc was announced by CoreOS along with rkt. OCI was announced at DockerConEU 2015 and included the founders of Appc as founding members. The CCNF is focused more at the orchestration level, addressing the issues of building containerised, microservice-based distributed systems at scale. Out of the work of Appc and OCI we've not yet seen truly interoperable, widely adopted standards; there has been some progress, but it's been slow and inter-stakeholder politics is evident. Let's hope this changes going forward.\n\nContainer Runtimes, Tooling and Services Round-Up\n\nContainer Runtimes\n\nDocker is clearly the most important container runtime available today. There are a few others popping up, including rkt from CoreOS and LXD from Canonical. LXD is unlikely to gain much traction outside of Canonical's sphere of influence (unless Microsoft and Canonical have more surprises in store for us - LXD on Windows perhaps?) but with rkt soon to be fully supported in Kubernetes, and increasingly popular as a suitable runtime for building platforms, it's one to watch. It's likely we'll see more container runtimes from other big companies.\n\nContainer Platforms\n\nThe Docker Platform - We talk about Docker as a container runtime, but the reality is that it is increasingly more of a platform. Docker wants complete vertical control over your container stack and it doesn't play well with OS init systems such as systemd. The Docker platform is made up of the following suite of products:\n\nDocker Machine - a tool for creating VMs with Docker installed, and configuring them with Swarm. It is also used to set environment variables to point your CLI client to different Docker APIs (e.g. on a VM).\n\nDocker Compose - known as Fig in the early days of Docker, Docker Compose is a tool for describing and launching applications made up of a number of containers that need to speak to each other.\n\nDocker Swarm - Swarm is Docker's clustering and orchestration solution. You can think of Swarm as a proxy for the Docker API that sits in front of a number of Docker engine APIs, and the underlying clustering mechanism.\n\nDocker Toolbox - Toolbox is a handy Windows &amp; Mac installer to help you get started with Docker Machine, Compose and Swarm. You don't need it for Linux.\n\nDocker Cloud - born out of the acquisition of Tutum, Docker Cloud gives IT Ops teams the ability to manage and deploy their Dockerised distributed applications. Works with existing IaaS providers as well as on-prem.\n\nDocker Universal Control Plane - Docker's home-grown web UI for managing Docker hosts and deploying containers, as part of Docker Datacenter.\n\nDocker Trusted Registry - store your Docker images on-premise using this private registry.\n\nDocker Datacenter - this is Docker's complete on-prem package, including both Universal Control Plane and Trusted Registry.\n\nThe core platform comprised of Docker Engine, Compose and Swarm are what we can compare to other platforms such as Kubernetes.\n\nKubernetes - The big name in container platforms is Google's open-source Kubernetes project. Modeled on what Google have learned from running containers in production for years, Kubernetes presents a cluster of machines as a single system to simplify container operations. A number of higher-level systems have been built on top of it, including Google Container Engine, Tectonic from CoreOS and container PaaS Deis.\n\nApache Mesos - Mesos describes itself as a distributed systems kernel. By abstracting individual machine resources it presents a scalable and powerful data-centre abstraction that can run many kinds of jobs, containers being just one of them.\n\nHosted Container Services\n\nGCE - Newer than Amazon's offering, but far more feature-rich and powerful. GCE is essentially Google-hosted Kubernetes.\n\nAmazon ECS - Mature, easy to use, yet rudimentary, ECS is suitable for simple services and for teams that are relatively new to containers.\n\nContainer-Based Operating Systems\n\nCoreOS - The original bare-bones, built-for-containers operating system, and still miles ahead of the competition. The team at CoreOS are behind an impressive array of projects- Etcd, rkt, Clair, Fleet, Flannel and Quay.io.\n\nThere are now a few other container-based operating systems. Project Atomic, sponsored by RedHat, brings familiar tools like RPM, not to mention a name people know and trust; RancherOS takes the \"minimalist\" concept to the extreme, containerising pretty much everything in the OS; Canonical and VMWare have also joined the party with Snappy and Photon, respectively. There is also the Data Center Operating System from Mesosphere, which is quite a different beast.\n\nNetworking, Monitoring and Storage\n\nThe container community has spawned uncountable projects and products in order to fill in the gaps in what is still relatively rudimentary in terms of a complete user experience. At the forefront of these are networking and monitoring solutions from Weaveworks, storage solutions from ClusterHQ and monitoring tools from just about everyone, with Sysdig perhaps being the most exciting.\n\nSee You in San Francisco on the 15th of April!\n\nJust when you think you're getting your head around the container ecosystem, a slew of new projects are announced. We've still not reached peak confusion in containers.\n\nWith talks from Google, Docker, Joyent, CoreOS, Deis, Netflix and more, what better way to join in the discussion and keep up to date than by attending Container Camp? It's not too late to pick up a ticket. Be sure to use our promo code \"YLDIO\" for $100 off until April 8th! I'm especially looking forward to the keynote from Alex Williams.\n\nHave questions about running containers in production? Just love talking about containers? YLD have been running containers in production since 2014. Come and say hi to me, Tom and Andy and we'd be delighted to have a chat."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Multiplexing Streams","description":"In the previous article of this series \"Using a Remote Emitter\", we showed you how we can easily propagate events between Node.js processes.\n\nThis next article builds on this, showing you how you to apply a","summary":"<p>In the previous article of this series \"Using a Remote Emitter\", we showed you how we can easily propagate events between Node.js processes.</p>\n\n<p>This next article builds on this, showing you how you to apply a duplex stream to it, inside it, transport several streams, and create independent channels</p>","date":"2016-03-23T15:27:41.000Z","pubdate":"2016-03-23T15:27:41.000Z","pubDate":"2016-03-23T15:27:41.000Z","link":"http://blog.yld.io/2016/03/23/multiplexing-streams/","guid":"557d9f31-307c-4621-a336-6a9fa9c4bdc7","author":"Pedro Teixeira","comments":null,"origlink":null,"image":{},"source":{},"categories":["node.js","networking","patterns"],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-3.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Multiplexing Streams"},"rss:description":{"@":{},"#":"<p>In the previous article of this series \"Using a Remote Emitter\", we showed you how we can easily propagate events between Node.js processes.</p>\n\n<p>This next article builds on this, showing you how you to apply a duplex stream to it, inside it, transport several streams, and create independent channels</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/03/23/multiplexing-streams/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"557d9f31-307c-4621-a336-6a9fa9c4bdc7"},"rss:category":[{"@":{},"#":"node.js"},{"@":{},"#":"networking"},{"@":{},"#":"patterns"}],"dc:creator":{"@":{},"#":"Pedro Teixeira"},"rss:pubdate":{"@":{},"#":"Wed, 23 Mar 2016 15:27:41 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-3.png","medium":"image"}},"content:encoded":{"@":{},"#":"In the previous article of this series \"Using a Remote Emitter\", we showed you how we can easily propagate events between Node.js processes.\n\nThis next article builds on this, showing you how you to apply a duplex stream to it, inside it, transport several streams, and create independent channels that use only one connection.\n\nYou can find the code for this article in this Github repo.\n\nEnjoy!\n\n\n\nLet's say now that your refrigerator door sensor server has been proved a success, and that you want to extend it. For instance, you would like to sense the refrigerator temperature and make that data available to the clients connecting to the server.\n\nBut there's a problem: you will need to add more event types and also change the ones that exist: open and close would have to be changed to door open and door close and you will also have to add temperature reading or something similar.\n\nThese events should be separated, i.e. transported in different streams, to make the server and client implementations easier and more adaptable to change. We could create different sub-streams on the server -> client stream, one for each device: one for the door sensor, another for the thermometer, and yet another should you choose to add yet another sensor to the refrigerator.\n\nFortunately this can be easily accomplished by using the mux-demux npm package: it lets us:\n\n\non the server side, multiplex several streams into one stream;\non the client side, do the inverse: de-multiplex that stream into separate streams to treat them individually\n\n\nExample: The fridge server\n\nLet's then introduce this change onto our server, which now gets properly named fridge_server:\n\nfridge_server.js:\n\nvar net = require('net');  \nvar DuplexEmitter = require('duplex-emitter');  \nvar Mux = require('mux-demux');\n\nvar server = net.createServer();\n\nserver.on('connection', handleConnection);\n\nserver.listen(8000, function() {  \n  console.log('door server listening on %j', server.address());\n});\n\n\n// sensors\n\nvar sensors = [  \n  {\n    name: 'door',\n    events: ['open', 'close'],\n    emitter: require('./door'),\n    remotes: {},\n    nextId: 0,\n    lastEvent: undefined\n  },\n  {\n    name: 'temperature',\n    events: ['reading'],\n    emitter: require('./thermometer'),\n    remotes: {},\n    nextId: 0,\n    lastEvent: undefined\n  },\n];\n\n\n// handle connections\n\nfunction handleConnection(conn) {  \n  var mx = Mux();\n\n  conn.on('error', onError);\n  mx.on('error', onError);\n\n  conn.pipe(mx).pipe(conn);\n\n  sensors.forEach(attachSensor);\n\n  function attachSensor(sensor) {\n    var stream = mx.createWriteStream(sensor.name);\n    var remoteEmitter = DuplexEmitter(stream);\n\n    stream.once('close', onClose);\n    stream.on('error', onError);\n    mx.on('error', onError);\n\n    // add remote to sensor remotes\n    var id = ++ sensor.nextId;\n    sensor.remotes[id] = remoteEmitter;\n\n    if (sensor.lastEvent) {\n      remoteEmitter.emit.apply(remoteEmitter, sensor.lastEvent);\n    }\n\n    function onClose() {\n      delete sensor.remotes[id];\n    }\n\n  }\n\n  function onError(err) {\n    conn.destroy();\n    console.error('Error on connection: ' + err.message);\n  }\n\n}\n\n\n/// broadcast all sensor events to connections\n\nsensors.forEach(function(sensor) {  \n  sensor.events.forEach(function(event) {\n\n    // broadcast all events of type `event`\n    sensor.emitter.on(event, broadcast(event, sensor.remotes));\n\n    // store last event on `sensor.lastEvent`\n    sensor.emitter.on(event, function() {\n      var args = Array.prototype.slice.call(arguments);\n      args.unshift(event);\n      sensor.lastEvent = args;\n    });\n  });\n});\n\nfunction broadcast(event, remotes) {  \n  return function() {\n    var args = Array.prototype.slice.call(arguments);\n    args.unshift(event);\n\n    Object.keys(remotes).forEach(function(emitterId) {\n      var remote = remotes[emitterId];\n      remote.emit.apply(remote, args);\n    });\n\n  };\n}\n\n\nThe biggest change from the previous version is that we now have a sensor definition where we store:\n\n\nname: the name of the sensor\nevents: which events the sensor will emit that we should propagate to the clients\nemitter: which event emitter object emits the events for this sensor\nremotes: the remote event emitters we should broadcast events to\nnextId: the id we will assign to the next remote emitter\nlastEvent: the last emitted event for this sensor, to be emitted once a client connects\n\n\nOnce the server gets a connection, we attach all of the defined sensors, storing and propagating all the interesting sensor events to the remote emitters. Each remote emitter has its own stream, which we create using mx.createWriteStream(name). This stream bears the same name as the sensor, which allows us to differentiate the streams on the client side.\n\nThe fridge client\n\nThe client now has to handle multiple streams inside the main one – let's see what needs to change:\n\nfridge_client.js:\n\nvar Mux = require('mux-demux');  \nvar Reconnect = require('reconnect-net');  \nvar DuplexEmitter = require('duplex-emitter');\n\nvar hostname = process.argv[2];  \nvar port = Number(process.argv[3]);  \nvar doorTimeoutSecs = Number(process.argv[4]);  \nvar maxTemperature = Number(process.argv[5]);\n\nvar reconnect = Reconnect(onConnect).connect(port, hostname);\n\nvar sensors = {  \n  'door': handleDoor,\n  'temperature': handleTemperature\n};\n\nfunction onConnect(conn) {  \n  var mx = Mux(onStream);\n  conn.pipe(mx).pipe(conn);\n\n  function onStream(stream) {\n    var handle = sensors[stream.meta];\n    if (! handle) {\n      throw new Error('Unknown stream: %j', stream.meta);\n    }\n    handle(DuplexEmitter(stream));\n  }\n}\n\n/// Door\n\nfunction handleDoor(door) {  \n  var timeout;\n  var warned = false;\n\n  door.on('open', onDoorOpen);\n  door.on('close', onDoorClose);\n\n  function onDoorOpen() {\n    timeout = setTimeout(onDoorTimeout, doorTimeoutSecs * 1e3);\n  }\n\n  function onDoorClose() {\n    if (warned) {\n      warned = false;\n      console.log('closed now');\n    }\n    if (timeout) {\n      clearTimeout(timeout);\n    }\n  }\n\n  function onDoorTimeout() {\n    warned = true;\n    console.error(\n      'DOOR OPEN FOR MORE THAN %d SECONDS, GO CLOSE IT!!!',\n      doorTimeoutSecs);\n  }\n}\n\n\n/// Temperature\n\nfunction handleTemperature(temperature) {  \n  temperature.on('reading', onTemperatureReading);\n\n  function onTemperatureReading(temp, units) {\n    if (temp &gt; maxTemperature) {\n      console.error('FRIDGE IS TOO HOT: %d %s', temp, units);\n    }\n  }\n}\n\n\nThis client now accepts an additional argument – the maximum temperature it allows before emitting a warning:\n\nvar maxTemperature = Number(process.argv[5]);  \n\n\nAlso, it has a definition of all the expected sensors and their function handlers:\n\nvar sensors = {  \n  'door': handleDoor,\n  'temperature': handleTemperature\n};\n\n\nWhen we connect to the server we need to instantiate a Mux-Demux stream, setting a stream handler: Mux-Demux will emit a connection event on each sub-stream. Each one of the sub-streams has a new attribute named meta, bearing the same name we gave it on the server-side:\n\nfunction onConnect(conn) {  \n  var mx = Mux(onStream);\n  conn.pipe(mx).pipe(conn);\n\n  function onStream(stream) {\n    var handle = sensors[stream.meta];\n    if (! handle) {\n      throw new Error('Unknown stream: %j', stream.meta);\n    }\n    handle(stream);\n  }\n}\n\n\nOnce we have the handler we can call create the duplex stream from the sub-stream and hand it off. The handleDoor function implements the same warning logic as before, and the new handleTemperature emits a warning if the emitted temperature is higher than the given maximum temperature.\n\nLet's test this server and client now. First off, start the server:\n\n$ node fridge_server\n\n\nIn another shell window, start the client:\n\n$ node fridge_client localhost 8000 1 10\n\n\nGiven the arguments and the randomness of the server-side sensor emitters, you should soon start to see some warnings like this:\n\nFRIDGE IS TOO HOT: 17.606447436846793 C  \nDOOR OPEN FOR MORE THAN 1 SECONDS, GO CLOSE IT!!!  \nFRIDGE IS TOO HOT: 13.070351709611714 C  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \nFRIDGE IS TOO HOT: 14.822801598347723 C  \nclosed now  \nFRIDGE IS TOO HOT: 15.487561323679984 C  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \nclosed now  \nFRIDGE IS TOO HOT: 13.501591393724084 C  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \nclosed now  \n\n\nNext article: Remote Procedure calls\n\nIn the next article for this series we'll be taking a look at how you can perform remote procedure calls on top of a channel so that we can control our office fridge!\n\n\n\nThis article was extracted from the Networking Patterns, a book from the Node Patterns series."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"QCon London 2016","description":"On March 7-9 in London I attended my first QCon. Organised by InfoQ, QCon is now in its tenth year and appears to be going from strength to strength.\n\nQCon is a developer conference that aims","summary":"<p>On March 7-9 in London I attended my first QCon. Organised by InfoQ, QCon is now in its tenth year and appears to be going from strength to strength.</p>\n\n<p>QCon is a developer conference that aims to appeal to a broad range of developers; from enterprise Java developers to cutting-edge</p>","date":"2016-03-15T15:02:01.000Z","pubdate":"2016-03-15T15:02:01.000Z","pubDate":"2016-03-15T15:02:01.000Z","link":"http://blog.yld.io/2016/03/15/qcon-london-2016/","guid":"247e086a-448a-4e3c-b218-97e00c21b298","author":"Luke Bond","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"QCon London 2016"},"rss:description":{"@":{},"#":"<p>On March 7-9 in London I attended my first QCon. Organised by InfoQ, QCon is now in its tenth year and appears to be going from strength to strength.</p>\n\n<p>QCon is a developer conference that aims to appeal to a broad range of developers; from enterprise Java developers to cutting-edge</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/03/15/qcon-london-2016/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"247e086a-448a-4e3c-b218-97e00c21b298"},"dc:creator":{"@":{},"#":"Luke Bond"},"rss:pubdate":{"@":{},"#":"Tue, 15 Mar 2016 15:02:01 GMT"},"content:encoded":{"@":{},"#":"On March 7-9 in London I attended my first QCon. Organised by InfoQ, QCon is now in its tenth year and appears to be going from strength to strength.\n\nQCon is a developer conference that aims to appeal to a broad range of developers; from enterprise Java developers to cutting-edge computer science \nresearch.\n\nYLD's very own Pedro Teixeira and Igor Soarez did a double-act talk about Node.js on the Full-Stack JavaScript track.\n\nIn this blog post I'll cover the talks that I attended and give my overall impressions of the conference.\n\nThe Talks\n\nDay 1\n\nKeynote: Unevenly Distributed\n\nby Adrian Colyer\n\nAdrian is the author of The Morning Paper, a daily digest of research paper summaries. Adrian spoke about what he has learned by reading and summarising a paper a day for over a year. He posited that papers A. make us think, B. raise our expectations of what's possible, C. share applied lessons with us, D. allow us to participate in the \"Great Conversation\" through the various references cited in the papers we read and E. give us a glimpse into the future (which is already here, but not evenly distributed).\n\nThis was one of the best talks I've seen in a long time. Adrian is a well-informed and a great speaker.\n\nLove Failure &amp; Embrace Fallout\n\nby Gavin Stevenson\n\nI'm interested in distributed systems and the complex problems around testing them, breaking them and making them robust in unreliable network scenarios. The title of this talk therefore appealed to me greatly. However, after extolling the virtues of designing for failure, the focus of this talk shifted to William Hill's R&amp;D department.\n\nThe talk was well delivered but I didn't learn anything from it and feel it didn't deliver on the title. Perhaps it was only guilty of a misleading talk title.\n\nUsing Technology as a Blind Long Distance Runner\n\nby Simon Wheatcroft\n\nSimon taught himself to run, blind, using the RunKeeper running app for his phone. Starting on a football field, using the audio feedback, he moved to closed suburban roads and then open motorways. Astonishingly, he uses only the combination of route memorisation through under-foot feel, the pain of running into things, and the audio feedback of RunKeeper. He has since run marathons and is about to participate in a desert race using custom technology he has produced with the help of IBM.\n\nResilient Predictive Data Pipelines\n\nby Sid Anand\n\nBuilding back-end systems that scale well and don't lose messages is hard. Sid presented some AWS-based examples of how to do this using a combination of SNS and SQS. An informative and well-delivered talk.\n\nJava 9 - The (G1) GC Awakens!\n\nby Monica Beckwith\n\nThis was a deep dive into the internals and minutiae of the new garbage collector in Java 9. I was hoping for something a little lighter and maybe talking about some of the other new features of Java 9. \n\nAs a recovering ex-Java developer, the level of detail was not of interest to me. An impressive and uncompromising talk by an accomplished speaker, but just not for me.\n\nDay 2\n\nEffortless Eventual Consistency with Weave Mesh\n\nby Peter Bourgon &amp; Matthias Radestock\n\nThe \"Modern CS in the Real World\" track was the place to go for mind-bending, cutting-edge computer science talks, and this talk was no exception. Peter and Matthias from Weaveworks talked about Weave Mesh, a library they abstracted from the internals of Weave itself that does eventual consistency over unreliable networks. They gave an applied example of building a Raft implementation on top of Mesh, which is pretty impressive, and the ease of building it shows the strength of their code and the interface and abstractions they chose.\n\nPatterns in a Containerised World\n\nby Matthias Luebken\n\nMatthias, previously of Giantswarm, set out something of a manifesto for container best practices. Think of it as the \"12 factor manifesto\" for containers. We've been working with containers for long enough now to have ironed some things out and to have begun to agree on some best practices. Matthias took us through his version of those, which he is sharing as a starting point for community collaboration. A good start and I hope it bears fruit in the container community.\n\nObserve, Enhance, &amp; Control: VMs to Containers\n\nby Mitchell Hashimoto\n\nMitchell delivered a broad and general talk, walking us through the history from VMs to containers and where we are today. Mitchell is a seasoned speaker and his talks are always enjoyable, but I found this to be old ground for me and perhaps directed at container newcomers.\n\nNot Quite so Broken TLS Using Unikernels\n\nby Anil Madhavapeddy\n\nI returned to the CS track to feel overwhelmed all over again, yet Anil managed to make a very difficult subject enjoyable and more-or-less understandable. Anil showed us how much of a mess the C code for very important libraries such as SSL can be, and how remarkable it is that they work as well as they do, despite the constant stream of exploits being announced. He proposes unikernels as a technology to enable us to write minimal library operating systems for our apps, rewriting these low-level libraries in type-safe, memory-safe and testable high-level languages to make the internet more secure.\n\nGreat talk.\n\nBurnout\n\nby John Willis\n\nJohn Willis from Docker shared a moving story about the suicide of a friend who worked in DevOps, and went on to share some statistics and studies about burnout and some resources for those who need help.\n\nThe take-away is: talk, listen, ask if people are okay and, above all, care. Also check out the Maslach Burnout Inventory: take the test and see if you are at risk.\n\nGreat talk.\n\nBuild, Ship and Run Unikernels\n\nby Justin Cormack\n\nAn entertaining and informative talk about unikernels as the logical extension of containers at Docker. Justin brings his dry humour and love of history to the subject and sheds some light on where Docker is heading with this.\n\nQCon Architects Dinner\n\nEvening social event\n\nThis year QCon ran an event called \"Architects dinner\", a new idea to get like-minded people together to talk about common interests. Each attendee gave their interests in advance and we were hand-matched into tables of six.\n\nUnfortunately I was the only non enterprise Java developer at the table. Aside from this, I met some nice people and had an enjoyable meal, but I can't say it was the success I'd hoped for.\n\nDay 3\n\nMeet the Node.js Anti-Patterns\n\nby Igor Soarez &amp; Pedro Teixeira\n\nYLD's co-founder and CTO Pedro Teixeira was joined by fellow YLDer Igor Soarez in a Node.js double-act discussing anti-patterns and bad practices in Node.js development. Built around a narrative of Jane, a Java developer doing her first Node.js project, we learned about callback hell, code style, error handling and some architecture and scalability challenges. Packed full of lessons, this will be one of those videos you go back and watch many times to extract as much as possible from it.\n\nPedro gave away copies of his Node Patterns books, which disappeared quickly. If you missed out and would like the books, head to the website.\n\nIf you have any questions about anything in the talk, please get in touch.\n\nRust: Systems Programming for Everyone\n\nby Felix Klock\n\nReturning to the CS track for more confusion, I was again pleased that this talk was very clear and understandable. Felix is an energetic and enthusiastic speaker and very engaging. Not being familiar with Rust some of the details were over my head but I was able to get the gist of most of it. Enough to tell me that I need to go and learn some Rust, it looks great.\n\nJS Everywhere\n\nby Matteo Collina\n\nNowadays you can run JavaScript almost anywhere, making it an increasingly popular language for IoT. Matteo talked about Fuge, Seneca, Vidi, Mosca and iot-system; a suite of IoT related tooling he has been working on this past year. A Matteo Collina talk wouldn't be complete without an ambitious live demo, and this was no exception.\n\nDesigning a Microservices Architecture with Node.js\n\nby Pascal Laenen\n\nPascal allowed us a peek inside Thomas Cook's innovation over the past 12 months: introducing microservices and serverless architecture with AWS Lambda into Thomas Cook. YLD have been a partner in developing Pascal's vision for the future of software engineering at Thomas Cook, and it was great to see it so well articulated on stage at QCon.\n\nConclusion\n\nQCon strives to appeal to a broad range of technologies and levels of developer expertise. Most of the talks reaffirmed some of my previous knowledge, but I was challenged and intrigued by the more advanced computer science talks. Overall I found it really valuable to get the \"lay of the land\", to see what technologies people are interested in today and using in production. It is also a good opportunity to meet like-minded people and \"network\".\n\n\n\nThe following content is the thoughts and reflections of Luke Bond. YLD does not hold these identical beliefs, and the expressed opinions are exclusively his own."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Using a Remote Emitter","description":"In a previous article we covered the Event Emitter pattern, which allows you to somewhat detach the producer of events from the consumer. This pattern may also be useful for providing a means of communication","summary":"<p>In <a href=\"http://blog.yld.io/2015/12/15/using-an-event-emitter\">a previous article</a> we covered the Event Emitter pattern, which allows you to somewhat detach the producer of events from the consumer. This pattern may also be useful for providing a means of communication between two processes: process A connects to process B, and they can both send events</p>","date":"2016-03-14T15:16:57.000Z","pubdate":"2016-03-14T15:16:57.000Z","pubDate":"2016-03-14T15:16:57.000Z","link":"http://blog.yld.io/2016/03/14/using-a-remote-emitter/","guid":"d344a0bb-a1dc-447f-8250-2d70b88f1d3f","author":"Pedro Teixeira","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-4.png","type":"image","length":null}],"rss:@":{},"rss:title":{"@":{},"#":"Using a Remote Emitter"},"rss:description":{"@":{},"#":"<p>In <a href=\"http://blog.yld.io/2015/12/15/using-an-event-emitter\">a previous article</a> we covered the Event Emitter pattern, which allows you to somewhat detach the producer of events from the consumer. This pattern may also be useful for providing a means of communication between two processes: process A connects to process B, and they can both send events</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/03/14/using-a-remote-emitter/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"d344a0bb-a1dc-447f-8250-2d70b88f1d3f"},"dc:creator":{"@":{},"#":"Pedro Teixeira"},"rss:pubdate":{"@":{},"#":"Mon, 14 Mar 2016 15:16:57 GMT"},"media:content":{"@":{"url":"http://blog.yld.io/content/images/2016/05/networking-patterns_png-4.png","medium":"image"}},"content:encoded":{"@":{},"#":"In a previous article we covered the Event Emitter pattern, which allows you to somewhat detach the producer of events from the consumer. This pattern may also be useful for providing a means of communication between two processes: process A connects to process B, and they can both send events to each other. When one of the sides receives an event, it may choose to simply ignore it (by not listening to that event), or to handle it by attaching an event handler — much like the Event Emitter pattern.\n\nLet's see this in action. Here is a use case:\n\n\n  Your office has a refrigerator that everyone uses. But now and then someone forgets to close the refrigerator door, leaving it to warm up, thus endangering the safety of all the contained food. This just can't be tolerated. As the smart person that you are, you attach a sensor to the door and a small computer (let's say a Raspberry Pi) connected to the office network. This computer can run Node.js and you take that chance to create a Node.js-powered TCP server that notifies of changes in the door state.\n\n\nThe Server\n\nLet's then create the door server:\n\ndoor_server.js:\n\nvar net = require('net');  \nvar DuplexEmitter = require('duplex-emitter');  \nvar door = require('./door');\n\nvar server = net.createServer();\n\nserver.on('connection', handleConnection);\n\nserver.listen(8000, function() {  \n  console.log('door server listening on %j', server.address());\n});\n\n\n/// store door state\n\nvar open = false;  \nvar lastEventTime;\n\ndoor.on('open', onOpen);  \ndoor.on('close', onClose);\n\nfunction onOpen(time) {  \n  open = true;\n  lastEventTime = time;\n}\n\nfunction onClose(time) {  \n  open = false;\n  lastEventTime = time;\n}\n\n\n// handle connections\n\nvar nextId = 0;  \nvar emitters = {};\n\nfunction handleConnection(conn) {  \n  var remoteEmitter = DuplexEmitter(conn);\n  var id = ++ nextId;\n  emitters[id] = remoteEmitter;\n\n  conn.once('close', onClose);\n  conn.on('error', onError);\n\n  if (lastEventTime) {\n    remoteEmitter.emit(open ? 'open' : 'close', lastEventTime);\n  }\n\n  function onClose() {\n    delete emitters[id];\n  }\n\n  function onError(err) {\n    console.error('Error on connection: ' + err.message);\n  }\n}\n\n\n/// broadcast door events\n\ndoor.on('open', broadcast('open'));  \ndoor.on('close', broadcast('close'));\n\nfunction broadcast(event) {  \n  return function() {\n    var args = Array.prototype.slice.call(arguments);\n    args.unshift(event);\n\n    Object.keys(emitters).forEach(function(emitterId) {\n      var emitter = emitters[emitterId];\n      emitter.emit.apply(emitter, args);\n    });\n\n  };\n}\n\n\nHere, as in previous examples, we instantiate a server and make it listen on TCP port 8000. The handleConnection function is now very different from what we've seen so far.\n\nWe start by instantiating a remote emitter by passing the connection to the duplex-emitter constructor. This gives us an object which, when you emit events on it, it transmits that event to the connection instead of emitting it locally. It's also duplex, which means that it will emit events that are coming from the other end of the connection.\n\nWe're using a local module (which we haven't introduced yet) named door. This module exposes a Singleton object that represents the door sensor and is itself an event emitter. This door event emitter then emits events of the types open and close. Every time the door emits one of these events, we need to transmit it to each and every one of the connected clients. This means that we then need to track each one of the remote emitters in a global object so that we can later send it messages.\n\nThe broadcast function is then used to broadcast each one of the event types to all registered emitters.\n\nWhen a connection closes we remove that emitter from the list of emitters:\n\nfunction onClose() {  \n  delete emitters[id];\n}\n\n\nWe still need one missing piece, the door:\n\ndoor.js:\n\nvar EventEmitter = require('events').EventEmitter;\n\nvar door = new EventEmitter();\n\nmodule.exports = door;\n\nvar open = false;\n\nfunction emitLater() {  \n  setTimeout(function() {\n\n    open = ! open; // flip state\n    var event = open ? 'open' : 'close';\n    door.emit(event, Date.now());\n\n    emitLater();\n\n  }, Math.floor(Math.random() * 5000));\n}\n\nemitLater();  \n\n\nAs I mentioned, this local door module exports an event emitter that emits close and open events. In this implementation these events are fake, randomly emitted every five seconds or less.\n\nLet's then test this server by starting it up:\n\n$ node door_server\ndoor server listening on {\"address\":\"0.0.0.0\",\"family\":\"IPv4\",\"port\":8000}  \n\n\nLet's then connect to it using a command-line tool like netcat or telnet:\n\n$ nc localhost 8000\n\n\nYou should start seeing these JSON strings being printed:\n\n[\"open\",1407334342639]\n[\"close\",1407334347577]\n[\"open\",1407334347694]\n[\"close\",1407334350693]\n[\"open\",1407334352930]\n[\"close\",1407334353093]\n[\"open\",1407334356678]\n[\"close\",1407334357292]\n[\"open\",1407334360026]\n[\"close\",1407334362703]\n[\"open\",1407334362930]\n[\"close\",1407334364680]\n[\"open\",1407334369361]\n\n\nThese are newline-separated JSON objects, each representing an event. In our case, we can see that the server is emitting the open and close events, passing along a timestamp representing when the respective event happened as the first and sole event argument.\n\nThe client\n\nWe can then create a Node.js client for the server. This client should be invoked by command line, accepting the server hostname and port. It should also accept a third option stating the maximum number of seconds the refrigerator door can stay open before it emits a warning.\n\ndoor_client.js:\n\nvar net = require('net');  \nvar DuplexEmitter = require('duplex-emitter');\n\nvar hostname = process.argv[2];  \nvar port = Number(process.argv[3]);  \nvar timeoutSecs = Number(process.argv[4]);\n\nvar timeout;  \nvar warned = false;\n\nvar conn = net.connect(port, hostname);  \nvar remoteEmitter = DuplexEmitter(conn);\n\nremoteEmitter.on('open', onOpen);  \nremoteEmitter.on('close', onClose);\n\nfunction onOpen() {  \n  timeout = setTimeout(onTimeout, timeoutSecs * 1e3);\n}\n\nfunction onClose() {  \n  if (warned) {\n    warned = false;\n    console.log('closed now');\n  }\n  if (timeout) {\n    clearTimeout(timeout);\n  }\n}\n\nfunction onTimeout() {  \n  warned = true;\n  console.error(\n    'DOOR OPEN FOR MORE THAN %d SECONDS, GO CLOSE IT!!!',\n    timeoutSecs);\n}\n\n\nThe door client connects to the server with the given hostname and port, and instantiates a duplex emitter from that connection. It listens for the open and close events. When the door opens, it sets a timeout for when to emit a warning. When the door closes, it clears the timeout. If the door doesn't close soon enough, the timeout triggers and the client prints a DOOR OPEN FOR MORE THAN x SECONDS, GO CLOSE IT!!! to the console.\n\nNow you can, with the server running on another window, start the door client:\n\n$ node door_client localhost 8000 1\n\n\nHere we're starting the door client and telling it that it should connect to the door server on localhost port 8000. We're also telling it that the door should remain open for a maximum of one second — just for testing purposes, since our fake door on the server is emitting open and close events less than five seconds apart.\n\nYou should then start seeing, printed in the client console:\n\nDOOR OPEN FOR MORE THAN 1 SECONDS, GO CLOSE IT!!!  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECONDS, GO CLOSE IT!!!  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECONDS, GO CLOSE IT!!!  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECONDS, GO CLOSE IT!!!  \nclosed now  \n\n\nMaking the client resilient to disconnects\n\nThere's at least one problem with this client: if the server disconnects for some reason (e.g. a restart), the client exits, and its exiting can go unnoticed. You can, of course, create a shell script that keeps the client running, but there's another option if you don't want to lose the client state: reconnect.\n\nTo reconnect we can use a package named reconnect-net, which is based on the generic reconnect-core, but specifically reconnects TCP connections. Let's install it:\n\n$ npm install reconnect-net\n\n\nHere is the modified door client that reconnects:\n\ndoor_client_reconnect.js:\n\nvar Reconnect = require('reconnect-net');  \nvar DuplexEmitter = require('duplex-emitter');\n\nvar hostname = process.argv[2];  \nvar port = Number(process.argv[3]);  \nvar timeoutSecs = Number(process.argv[4]);\n\nvar timeout;  \nvar warned = false;\n\nvar reconnect = Reconnect(onConnect).connect(port, hostname);\n\nreconnect.on('disconnect', function() {  \n  console.log('disconnected');\n});\n\nfunction onConnect(conn) {  \n  console.log('connected');\n  var remoteEmitter = DuplexEmitter(conn);\n\n  remoteEmitter.on('open', onOpen);\n  remoteEmitter.on('close', onClose);\n}\n\nfunction onOpen() {  \n  timeout = setTimeout(onTimeout, timeoutSecs * 1e3);\n}\n\nfunction onClose() {  \n  if (warned) {\n    warned = false;\n    console.log('closed now');\n  }\n  if (timeout) {\n    clearTimeout(timeout);\n  }\n}\n\nfunction onTimeout() {  \n  warned = true;\n  console.error(\n    'DOOR OPEN FOR MORE THAN %d SECONDS, GO CLOSE IT!!!',\n    timeoutSecs);\n}\n\n\nHere you can see that we're using reconnect-net to create a reconnect object, passing in the connection handler, and also asking it to connect to the given port and hostname. When the connection is established, our onConnect function gets called, and this is when we instantiate the remote emitter and start listening to the interesting remote events.\n\nWhen this connection fails, the reconnect object emits a disconnect event which we listen to, printing out a warning. The rest of the logic we've already seen in the previous version of the client.\n\nYou can test this client by starting it as we did previously:\n\n$ node door_client_reconnect.js localhost 8000 1\n\n\nEverything should behave as previously. You can now close down the server and restart it. You should then see a series of disconnect events, where reconnect is failing to connect.\n\n\n  Notice that the disconnect events are quite close together in time at the beginning, but the interval between them starts to grow with time. This is because, by default, reconnect-core uses exponential backoff when reconnecting and failing. You can configure several aspects of this backoff by supplying reconnect-net with some options.\n\n\nOnce the server is back up, the client should reconnect successfully and resume operations:\n\nconnected  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \ndisconnected  \ndisconnected  \ndisconnected  \ndisconnected  \ndisconnected  \ndisconnected  \nconnected  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \nclosed now  \nDOOR OPEN FOR MORE THAN 1 SECOND, GO CLOSE IT!!!  \n\n\nNext article: Multiplexing Streams\n\nLet's say now that your refrigerator door sensor server has been proved a success, and that you want to extend it. For instance, you would like to sense the refrigerator temperature and make that data available to the clients connecting to the server.\n\nBut there's a problem: you will need to add more event types and also change the ones that exist: open and close would have to be changed to door open and door close and you will also have to add temperature reading or something similar.\n\nIn the next article of this series we'll show you can create different channels inside one stream and treat them like individual streams.\n\n\n\nThis article was extracted from the Networking Patterns, a book from the Node Patterns series."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Getting Started with rkt","description":"This February, CoreOS announced that their rkt container runtime had graduated to version 1.0. rkt has come a long way since its initial announcement in December 2014, so now’s a good time to take a","summary":"<p>This February, CoreOS announced that their <a href=\"https://coreos.com/blog/rkt-hits-1.0.html\" target=\"_blank\">rkt container runtime had graduated to version 1.0</a>. rkt has come a long way since its initial announcement in December 2014, so now’s a good time to take a closer look and consider how it fits into the rapidly changing container ecosystem.</p>","date":"2016-03-03T17:20:00.000Z","pubdate":"2016-03-03T17:20:00.000Z","pubDate":"2016-03-03T17:20:00.000Z","link":"http://blog.yld.io/2016/03/03/getting-started-with-rkt/","guid":"dc9f1fee-b3b4-437a-9f37-cdb7974cc599","author":"Luke Bond","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Getting Started with rkt"},"rss:description":{"@":{},"#":"<p>This February, CoreOS announced that their <a href=\"https://coreos.com/blog/rkt-hits-1.0.html\" target=\"_blank\">rkt container runtime had graduated to version 1.0</a>. rkt has come a long way since its initial announcement in December 2014, so now’s a good time to take a closer look and consider how it fits into the rapidly changing container ecosystem.</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/03/03/getting-started-with-rkt/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"dc9f1fee-b3b4-437a-9f37-cdb7974cc599"},"dc:creator":{"@":{},"#":"Luke Bond"},"rss:pubdate":{"@":{},"#":"Thu, 03 Mar 2016 17:20:00 GMT"},"content:encoded":{"@":{},"#":"This February, CoreOS announced that their rkt container runtime had graduated to version 1.0. rkt has come a long way since its initial announcement in December 2014, so now’s a good time to take a closer look and consider how it fits into the rapidly changing container ecosystem.\n\nThis article is intended for people who are new to rkt but have some experience with Linux containers, e.g., with Docker. Throughout this post, I’ll be assuming you’re using rkt with systemd on Linux.\n\nRead more at http://blog.codeship.com/getting-started-rkt/"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Building a TCP service using Node.js","description":"Building a Web application using a framework like PHP, Django or Rails can quickly lead to monolithic applications in which you try to cram in every little bit of functionality.\n\nThis happens not because of the","summary":"<p>Building a Web application using a framework like PHP, Django or Rails can quickly lead to monolithic applications in which you try to cram in every little bit of functionality.</p>\n\n<p>This happens not because of the web framework itself, but because these frameworks are designed to build consumer-facing apps and</p>","date":"2016-02-23T11:24:30.000Z","pubdate":"2016-02-23T11:24:30.000Z","pubDate":"2016-02-23T11:24:30.000Z","link":"http://blog.yld.io/2016/02/23/building-a-tcp-service-using-node-js/","guid":"6ee5c664-f306-4cad-9cd3-36fece38bb00","author":"Pedro Teixeira","comments":null,"origlink":null,"image":{},"source":{},"categories":[],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Building a TCP service using Node.js"},"rss:description":{"@":{},"#":"<p>Building a Web application using a framework like PHP, Django or Rails can quickly lead to monolithic applications in which you try to cram in every little bit of functionality.</p>\n\n<p>This happens not because of the web framework itself, but because these frameworks are designed to build consumer-facing apps and</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/02/23/building-a-tcp-service-using-node-js/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"6ee5c664-f306-4cad-9cd3-36fece38bb00"},"dc:creator":{"@":{},"#":"Pedro Teixeira"},"rss:pubdate":{"@":{},"#":"Tue, 23 Feb 2016 11:24:30 GMT"},"content:encoded":{"@":{},"#":"Building a Web application using a framework like PHP, Django or Rails can quickly lead to monolithic applications in which you try to cram in every little bit of functionality.\n\nThis happens not because of the web framework itself, but because these frameworks are designed to build consumer-facing apps and make it more difficult to do networked apps.\n\nFortunately, Node.js makes it easy for us to build and consume networked apps. With little effort, you can spread the functionalities of the application that you're building into a set of processes that communicate with each other. By avoiding the \"large monolithic application\" anti-pattern from the start you can:\n\n\nimprove code mantainability by having smaller versioned services that do just one thing and do it well;\nhave fine-grained control over the scalability model of your application: not all services need to share the resource requirements, allowing you to adjust them independently;\nhave a more controlled failure mode: one process in one service can fail without bringing the whole system down.\n\n\nHaving one code repository per service can also help with:\n\n\nmaking the API boundaries more explicit: you have to document the API contract for every service;\nimproving the test coverage: since the code base for each service is small in comparison with the monolithic application, you can easily test whether the service is working or not before releasing a new version, by having automatic tests with significant code coverage.\n\n\n\n  This approach comes with a few attached prices, specifically, the need for  thorough release management, (in which you must manage all these modules, their versions, and their dependencies on other modules and versions). Additionally, having a set of different services that talk to each other also makes the system more challenging to deploy and manage.\n\n\nIn this series of articles I'll start the analysis of some networking patterns. In this particular article we'll start by building a simple TCP service, which we'll then make evolve throughout this and future articles.\n\nBuilding a TCP Service\n\nThe simplest form of a networking service we can start off with is TCP service. A TCP service enables another process to connect to this service and, once connected, to have a raw bi-directional stream of data.\n\nLet's implement the first form of this service:\n\nraw_server.js:\n\nvar net = require('net');\n\nvar server = net.createServer();  \nserver.on('connection', handleConnection);\n\nserver.listen(9000, function() {  \n  console.log('server listening to %j', server.address());\n});\n\nfunction handleConnection(conn) {  \n  var remoteAddress = conn.remoteAddress + ':' + conn.remotePort;\n  console.log('new client connection from %s', remoteAddress);\n\n  conn.on('data', onConnData);\n  conn.once('close', onConnClose);\n  conn.on('error', onConnError);\n\n  function onConnData(d) {\n    console.log('connection data from %s: %j', remoteAddress, d);\n    conn.write(d);\n  }\n\n  function onConnClose() {\n    console.log('connection from %s closed', remoteAddress);\n  }\n\n  function onConnError(err) {\n    console.log('Connection %s error: %s', remoteAddress, err.message);\n  }\n}\n\n\nHere we start by creating the server object by calling .createServer on the net module. This gives us a server object. After this we make it listen to port 9000, printing the server address once the server is listening.\n\nWe also bind the handleConnection function to the connection event. The server emits this event every time there is a new TCP connection made to the server, passing in the socket object.\n\nThis socket object can emit several events: it emits a data event every time data arrives from the connected peer; a close event once that connection closes; and an error event if an error happens on the socket.\n\nWhen the server gets data from a connection, it logs it to the console and  writes it back into the connection:\n\n  function onConnData(d) {\n    console.log('connection data from %s: %j', remoteAddress, d);\n    conn.write(d);\n  }\n\n\n\n  Copying the input into the output makes this server an \"echo\" server. This is not particularly useful at the moment, but we'll make this evolve.\n\n\nWe also catch all the error events the socket emits. Typically you will get ECONNRESET errors here when the socket connection has been abruptly closed by the other end. All socket errors end the connection, which means that the close event will be emitted next.\n\nYou can test this service by first starting the server:\n\n$ node raw_server.js\nserver listening to {\"address\":\"0.0.0.0\",\"family\":\"IPv4\",\"port\":9000}  \n\n\nOn a different terminal window you can then connect to our server using a command-line application like Telnet or Netcat:\n\n$ nc localhost 9000\nhey  \nhey  \n^C\n\n\nYou can now type and hit the ENTER key. You'll see every line you enter echoed by the server.\n\nAlso, you'll see on the server logs that the server gets raw buffer data, printing out each byte:\n\nnew client connection from 127.0.0.1:57590  \nconnection data from 127.0.0.1:57590: [104,101,121,10]  \n\n\nAn Example of a Simple Service\n\nFor the sake of giving an example, let's say that, instead of building an echo service, this service will expect UTF-8 strings and will uppercase all the characters.\n\ncapitalizing_server_01.js:\n\nvar net = require('net');\n\nvar server = net.createServer();  \nserver.on('connection', handleConnection);\n\nserver.listen(9000, function() {  \n  console.log('server listening to %j', server.address());\n});\n\nfunction handleConnection(conn) {  \n  var remoteAddress = conn.remoteAddress + ':' + conn.remotePort;\n  console.log('new client connection from %s', remoteAddress);\n\n  conn.setEncoding('utf8');\n\n  conn.on('data', onConnData);\n  conn.once('close', onConnClose);\n  conn.on('error', onConnError);\n\n  function onConnData(d) {\n    console.log('connection data from %s: %j', remoteAddress, d);\n    conn.write(d.toUpperCase());\n  }\n\n  function onConnClose() {\n    console.log('connection from %s closed', remoteAddress);\n  }\n\n  function onConnError(err) {\n    console.log('Connection %s error: %s', remoteAddress, err.message);\n  }\n}\n\n\nThis server code is a copy of the echo server with two small additions:\n\n\nWhen the client connects, we set the encoding to utf8. This makes the connection pass strings when emitting data events instead of raw buffers, allowing us to have a JavaScript string we can then transform.\nBefore we write that string back to the connection, we transform it to uppercase.\n\n\nNow, stop the previous server if you're still running it, and start this new one:\n\n$ node capitalizing_server_01.js\nserver listening to {\"address\":\"0.0.0.0\",\"family\":\"IPv4\",\"port\":9000}  \n\n\nYou can now connect to that server — also using either Telnet or Netcat — type some text, getting back the uppercased version:\n\n$ nc localhost 9000\nhello  \nHELLO  \nplease pass me the salt  \nPLEASE PASS ME THE SALT  \n\n\nA JSON Stream\n\nSo far our service has only dealt with strings, but for some applications we may need some more structured data. JSON is a common representation for structured data, providing just a few basic types: Booleans, strings, numbers, arrays and objects. What we want most of the time is to pass in JavaScript objects that represent complex commands, queries or results, one at a time, throughout time; but it happens that JSON isn't particularly stream-friendly: typically you want to transmit a complex structure to the other side – either an object or an array – and JSON is only valid once the whole object is transmitted.\n\nA slight alteration of the JSON protocol enables us to use streaming: if none of the transmitted objects, when serialised, contain new-line characters, we can safely separate each one of the main objects with a new-line character. Fortunately, the default JSON encoding function available in Node.js doesn't introduce any new-line character, and for those existing in strings, it encodes them with two characters: \"\\n\".\n\nIf you sent two objects down such a stream, they would be encoded for transmission like this:\n\n{\"a\":1,\"b\":true,c:\"this is a newline-terminated string\\n\"}\n{\"d\":2,\"e\":false,f:\"this is another newline-terminated string\\n\"}\n\n\nWe can then easily create a service that accepts and replies with this encoding by using the json-duplex-stream module. Using this module we can let it create two streams for us:\n\n\none through stream that parses raw data and outputs the contained JavaScript objects;\nanother through stream that accepts JavaScript objects and outputs them JSON-encoded, with a new-line character at the end.\n\n\nExample: An Events Gateway\n\nUsing this, we're going to create a TCP server that accepts events from domotic devices (like thermometers, barometers, presence sensors, lights, etc.). Here are some requirements for this service:\n\n\nEach device connects to this service and sends it events throughout time.\nThe gateway service saves each of these events into a persisted queue for further processing by other processes.\nEach received object represents an event, and each event has a unique id field.\nOnce an event is saved into the queue, the gateway replies with a message confirming that the event with the given id was saved.\n\n\nLet's build this system.\n\nFirst we have to install the json-duplex-stream package:\n\n$ npm install json-duplex-stream\n\n\ngateway_server.js:\n\nvar net = require('net');  \nvar server = net.createServer();  \nvar JSONDuplexStream = require('json-duplex-stream');\n\nvar Gateway = require('./gateway')\n\nserver.on('connection', handleConnection);  \nserver.listen(8000, function() {  \n  console.log('server listening on %j', server.address());\n});\n\nfunction handleConnection(conn) {  \n  var s = JSONDuplexStream();\n  var gateway = Gateway();\n  conn.\n    pipe(s.in).\n    pipe(gateway).\n    pipe(s.out).\n    pipe(conn);\n\n  s.in.on('error', onProtocolError);\n  s.out.on('error', onProtocolError);\n  conn.on('error', onConnError);\n\n  function onProtocolError(err) {\n    conn.end('protocol error:' + err.message);\n  }\n}\n\nfunction onConnError(err) {  \n  console.error('connection error:', err.stack);\n}\n\n\nAs in the previous examples, we're creating the server using net.createServer(). We're then requiring the json-duplex-stream module to be used later.\n\nThen we require the local gateway.js module, which implements our gateway logic.\n\nAfter that we set a server connection handler named handleConnection, which gets called when a client connects to our server. Once that happens, we instantiate a JSON Duplex Stream and a gateway, and wire up the connection to the input side of the JSON Duplex Stream. When this JSON Duplex Stream instance gets valid JSON over the connection, it outputs the parsed JavaScript objects, which we then pipe into our gateway instance.\n\nOur gateway instance is a transform stream, and every object it outputs is piped to the output side of the JSON Duplex Stream. This serialises each of the objects, outputting JSON strings that get piped back to the client. Here is a quick diagram of that flow:\n\nconnection (conn) =&gt; JSON Duplex Stream input handler (s.in) =&gt;  \nGateway (gateway) =&gt; JSON Duplex Stream output handler (s.out) =&gt; connection (conn)  \n\n\nAfter we set up this pipeline, we need to handle protocol errors by listening to errors emitted by the streams s.in and s.out. The first one emits errors if the input data is not valid JSON; and the second one can emit errors if it's unable to encode an object into JSON for some reason. We handle any of these errors by writing the error message into the connection and closing it.\n\nLet's then implement the core of our application, the gateway.js module:\n\ngateway.js:\n\nvar extend = require('util')._extend;  \nvar inherits = require('util').inherits;  \nvar Transform = require('stream').Transform;\n\nmodule.exports = Gateway;\n\ninherits(Gateway, Transform);\n\nvar defaultOptions = {  \n  highWaterMark: 10,\n  objectMode: true\n};\n\nfunction Gateway(options) {  \n  if (! (this instanceof Gateway)) {\n    return new Gateway(options);\n  }\n\n  options = extend({}, options || {});\n  options = extend(options, defaultOptions);\n\n  Transform.call(this, options);\n}\n\n\n/// _transform\n\nGateway.prototype._transform = _transform;\n\nfunction _transform(event, encoding, callback) {  \n  if (! event.id)\n    return handleError(new Error('event doesn\\'t have an `id` field'));\n\n  pushToQueue(event, pushed);\n\n  function pushed(err) {\n    if (err) {\n      handleError(err);\n    }\n    else {\n\n      reply = {\n        id: event.id,\n        success: true\n      };\n\n      callback(null, reply);\n    }\n  }\n\n  function handleError(err) {\n    var reply = {\n      id: event.id,\n      success: false,\n      error: err.message\n    };\n\n    callback(null, reply);\n  }\n};\n\n\n/// Fake push to queue\n\nfunction pushToQueue(object, callback) {  \n  setTimeout(callback, Math.floor(Math.random() * 1000));\n}\n\n\nOur gateway is a transform stream, inheriting from streams.Transform. As we saw, this stream is a duplex stream (accepts data and outputs data) and must implement the _transform method. In this method we get the event, which is a JavaScript object by now, and we take the chance to validate it and push it into the queue (using the fake pushToQueue function). If the push operation was successful we reply with an object that bears the same event id and a success attribute set to true, to indicate that the event was accepted. Otherwise, if there was an error, we write back a reply object containing the event id, the success attribute set to false, and an error message.\n\nWe can now test this service. Start by launching the server:\n\n$ node gateway_server\nserver listening on {\"address\":\"0.0.0.0\",\"family\":\"IPv4\",\"port\":8000}  \n\n\nWe can then use netcat or telnet to connect to the server using the command line:\n\n$ nc localhost 8000\n\n\nOnce we are connected we're acting like a device, and can start writing events as if one, hitting the Return key at the end of each one:\n\n{ \"when\": \"2014-08-06T13:36:31.735Z\", \"type\": \"temperature\", \"reading\": 23.4, \"units\": \"C\", \"id\": \"5f18453d-1907-48bc-abd2-ab6c24bc197d\" }\n\n\nFor each correct one you enter you should get back a confirmation:\n\n{\"id\":\"5f18453d-1907-48bc-abd2-ab6c24bc197d\",\"success\":true}\n\n\nNext article: Using a Remote Emitter\n\nIn a previous article we covered the Event Emitter pattern, which allows you to somewhat detach the producer of events from the consumer. This pattern may also be useful for providing a means of communication between two processes: process A connects to process B, and they can both send events to each other. \n\n\n\n\n  This article was extracted from the Networking Patterns, a book from the Node Patterns series.\n"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Squeeze the juice out of Node","description":"— an exploration of how Node.js handles HTTP connections —\n\nWhy is this Node service only handling X requests per second but neither memory, CPU nor network usage is saturated? How does Node handle HTTP connections? Can","summary":"<p>— <em>an exploration of how Node.js handles HTTP connections</em> —</p>\n\n<p>Why is this Node service only handling X requests per second but neither memory, CPU nor network usage is saturated? How does Node handle HTTP connections? Can it process more than one request at the same time for a single non-HTTP/</p>","date":"2016-02-08T14:52:29.000Z","pubdate":"2016-02-08T14:52:29.000Z","pubDate":"2016-02-08T14:52:29.000Z","link":"http://blog.yld.io/2016/02/08/squeeze-the-juice-out-of-node/","guid":"8ac2a035-3c4c-43fa-b07c-69868a3da8a9","author":"Igor Soarez","comments":null,"origlink":null,"image":{},"source":{},"categories":["performance","http"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Squeeze the juice out of Node"},"rss:description":{"@":{},"#":"<p>— <em>an exploration of how Node.js handles HTTP connections</em> —</p>\n\n<p>Why is this Node service only handling X requests per second but neither memory, CPU nor network usage is saturated? How does Node handle HTTP connections? Can it process more than one request at the same time for a single non-HTTP/</p>"},"rss:link":{"@":{},"#":"http://blog.yld.io/2016/02/08/squeeze-the-juice-out-of-node/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"8ac2a035-3c4c-43fa-b07c-69868a3da8a9"},"rss:category":[{"@":{},"#":"performance"},{"@":{},"#":"http"}],"dc:creator":{"@":{},"#":"Igor Soarez"},"rss:pubdate":{"@":{},"#":"Mon, 08 Feb 2016 14:52:29 GMT"},"content:encoded":{"@":{},"#":"— an exploration of how Node.js handles HTTP connections —\n\nWhy is this Node service only handling X requests per second but neither memory, CPU nor network usage is saturated? How does Node handle HTTP connections? Can it process more than one request at the same time for a single non-HTTP/2 connection? If these questions spark your curiosity, read on.\n\n\n\nConsider the HTTP hello world example from nodejs.org:\n\nconst http = require('http');  \nhttp.createServer((req, res) =&gt; {  \n  res.writeHead(200, { 'Content-Type': 'text/plain' });\n  res.end('Hello');\n}).listen(8888);\n\n\nLet's spin it up (I'm using Node v4.2.6) and run wrk2 — an HTTP benchmarking tool — against it:\n\n$ wrk2 -R 50000 -d 15s -t 4 -c 20 http://localhost:8888/\n\n\nThis tells wrk2 to try to push a rate of 50000 requests per second (-R) for 15 seconds (-d) using 4 threads (-t) and 20 persistent connections (-c).\n\nI like to do it all in a one-liner:\n\n$ node server.js &amp;; pid=$!; sleep 1; wrk2 -R 50000 -d 15s -t 4 -c 20 http://localhost:8888/; kill -HUP $pid\n\n\nThis starts the Node process, puts it in background, saves the process ID, waits 1 second so that our Node HTTP server can boot up, runs the benckmarking tool against it and finally kills the Node process. \n\nResult:\n\nRunning 15s test @ http://localhost:8888/  \n  4 threads and 20 connections\n  Thread calibration: mean lat.: 3809.653ms, rate sampling interval: 13459ms\n  Thread calibration: mean lat.: 3809.566ms, rate sampling interval: 13467ms\n  Thread calibration: mean lat.: 3808.688ms, rate sampling interval: 13459ms\n  Thread calibration: mean lat.: 3810.143ms, rate sampling interval: 13459ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     8.90s   953.41ms  10.75s    60.16%\n    Req/Sec        nan       nan   0.00      0.00%\n  212176 requests in 15.00s, 30.15MB read\nRequests/sec:  14145.13  \nTransfer/sec:      2.01MB  \n\n\nThat's roughly 14k requests per second (I'll use the term RPS henceforth). \n\n\n  If you really need to know, I ran this on a late 2013 MacBook Pro Retina with a 2.4 GHz Intel Core i5 and 8 GB 1600 MHz DDR3. The environment will be the same for the rest of the tests.\n\n\nAn HTTP endpoint that replies immediately without doing any I/O probably isn't a very good representation of how we frequently use Node for HTTP. The hello world example is not what we want to bench. Let's add a 50 millisecond delay to every response, pretending we're querying either a database, another service or a file.\n\nconst http = require('http');  \nhttp.createServer((req, res) =&gt; {  \n  setTimeout(reply, 50);\n  function reply() {\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    res.end('Hello');\n  }\n}).listen(8888);\n\n\nHow will this affect the benchmark? All we're doing is just delaying the responses. Node is non-blocking, it can go on and keep processing further requests, right? It just means that these responses get buffered for a bit before being sent out, right?\n\nRunning 15s test @ http://localhost:8888/  \n  4 threads and 20 connections\n  Thread calibration: mean lat.: 5011.480ms, rate sampling interval: 17907ms\n  Thread calibration: mean lat.: 5011.394ms, rate sampling interval: 17907ms\n  Thread calibration: mean lat.: 5011.168ms, rate sampling interval: 17907ms\n  Thread calibration: mean lat.: 5011.091ms, rate sampling interval: 17907ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    12.40s     1.42s   14.84s    57.97%\n    Req/Sec        nan       nan   0.00      0.00%\n  5504 requests in 15.00s, 800.88KB read\nRequests/sec:    366.90  \nTransfer/sec:     53.39KB  \n\n\n... No. Wow wait what? How did we go down from 14k to 366 RPS?\n\nWhat is Node doing? Let's check the CPU usage. To find out CPU usage I like to run either top, htop or even ps:\n\n$ ps -p $(pgrep -o -f 'node server') -o 'pid command pcpu pmem'\n\n\nWhen using ps I like to run it with watch so it keeps refreshing and I can just look at it any time:\n\n$ watch 'ps -p $(pgrep -o -f \"node server\") -o \"pid command pcpu pmem\"'\n\n\n\n  Running top -pid $(pgrep -o -f \"node server\") would also work, but I'd have to restart it every time after the node process, with watch + ps I can just leave it running and it picks up the latest Node process which fits nicely with my lazyness.\n\n\nHere's what we find with ps:\n\n\nwithout the delay: CPU usage is really high ~95%\nwith the 50ms delay in every response: CPU usage is very low, never above 10%\n\n\nMaybe we're capping the amount of work we're giving Node by having a small number of connections. We can tweak wrk2's parameters to confirm this.\n\nUpping the number of connections to 100 raises the RPS:\n\n$ node server.js &amp;; pid=$!; sleep 1; wrk2 -R 50000 -d 15s -t 4 -c 100 http://localhost:8888/; kill -HUP $pid\n[1] 33003\nRunning 15s test @ http://localhost:8888/  \n  4 threads and 100 connections\n  Thread calibration: mean lat.: 4874.590ms, rate sampling interval: 17530ms\n  Thread calibration: mean lat.: 4872.940ms, rate sampling interval: 17530ms\n  Thread calibration: mean lat.: 4427.471ms, rate sampling interval: 15908ms\n  Thread calibration: mean lat.: 4873.753ms, rate sampling interval: 17530ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    11.85s     1.42s   14.46s    60.16%\n    Req/Sec        nan       nan   0.00      0.00%\n  23791 requests in 15.01s, 3.38MB read\nRequests/sec:   1585.38  \nTransfer/sec:    230.68KB  \n\n\nAnd bringing it down to a single connection lowers it even further:\n\n$ node server.js &amp;; pid=$!; sleep 1; wrk2 -R 50000 -d 15s -t 1 -c 1 http://localhost:8888/; kill -HUP $pid\n[1] 33329\nRunning 15s test @ http://localhost:8888/  \n  1 threads and 1 connections\n  Thread calibration: mean lat.: 5035.888ms, rate sampling interval: 17989ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    12.51s     1.45s   14.98s    57.45%\n    Req/Sec        nan       nan   0.00      0.00%\n  278 requests in 15.05s, 40.45KB read\nRequests/sec:     18.47  \nTransfer/sec:      2.69KB  \n\n\nSo we've determined that the current bottleneck is the number of connections. But there's no reason our server shouldn't be able to process more requests, afterall, there's still CPU, memory, and network bandwidth available. How many connections do we need to make Node use all of it's host's resources? \nIn this particular machine 3000 connections seems to top out Node's CPU usage, and RPS up to 12.5K.\n\n$ node server.js &amp;; pid=$!; sleep 1; wrk2 -R 50000 -d 15s -t 4 -c 3000 http://localhost:8888/; kill -HUP $pid\n[1] 40861\nRunning 15s test @ http://localhost:8888/  \n  4 threads and 3000 connections\n  Thread calibration: mean lat.: 3548.946ms, rate sampling interval: 14098ms\n  Thread calibration: mean lat.: 3546.422ms, rate sampling interval: 14147ms\n  Thread calibration: mean lat.: 3568.487ms, rate sampling interval: 14196ms\n  Thread calibration: mean lat.: 3601.093ms, rate sampling interval: 14196ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     8.60s     1.56s   13.12s    81.97%\n    Req/Sec        nan       nan   0.00      0.00%\n  188346 requests in 15.00s, 26.76MB read\n  Socket errors: connect 0, read 952, write 0, timeout 1570\nRequests/sec:  12558.20  \nTransfer/sec:      1.78MB  \n\n\n\n  To be able to run this I had to change the operating system's configuration value for the limit of open files — each open socket means another open file.\n\n\nIn a production setup Node is likely sitting behind a load balancer or a reverse proxy. Things like the 3-way-handshake and slow-start are performance costs in the form of extra roundtrips and reduced bandwidth that we need to pay everytime the underlying TCP connection in HTTP requests is established. So keeping connections open — using persistent HTTP connections — is likely something that the balancer will do.\n\nThe benchmarking tool we used, wrk2 also makes use of persistent HTTP connections for the same reason, that's why we specify the -c parameter.  With this in mind, it's obvious why CPU was low on the second test: wrk2 was waiting on one request to complete before sending out another one. And now that we know that, let's try to make sense of those numbers we got.\n\nExplaining the drop\n\nIn our first test we got an average of 14145.13 RPS using 20 connections, that means an average of 707.26 RPS (14145.13 / 20) per connection, or an average of 1.4139 milliseconds per request (1000 / 707.26).\n\nIn the second test, we added a 50 millisecond delay, so each request should then take 51.4139 (50 + 1.4139) milliseconds to be served. For a single connection that means 19.45 RPS per connection (1000 / 51.4139), or for all 20 connections, 389 RPS (20 * 19.45).\n\nThe numbers we actually hit were 366.90 RPS for 20 connections which is pretty close to 389 RPS, and later with a single connection we hit 18.47 RPS which also is pretty close to 19.45 RPS.\n\nScaling\n\nAssuming the backend — whatever service or database that's being used in the request — isn't a bottleneck, as long as there's available CPU, available memory and available network bandwidth the number of RPS can be increased with the number of connections.\n\nBut what's the right number of connections for my Node service? In a classical threadpool-based IO-blocking webserver, that number should be close to one per thread. In Node, with non-blocking IO, the answer is, it depends how long your Node service is waiting for IO to complete. The longer the wait, the more connections you need to saturate resources.\n\nIn the test we just saw, to saturate a single Node process that adds 50 milliseconds of IO to every request we needed 3k connections. Running on a similar environment but with an 8-CPU host we would want 7 more Node processes to fully leverage all available cores. We would then need 24k connections to saturate all 8 Node processes.\n\nCan we improve this?\nIs there a way to make Node handle more requests in parallel without having to keep extra connections open?\n\nEnter HTTP pipelining\n\nPersistent connections are achieved by consent of both client and server. HTTP/1.1 assumes every connection is persistent — i.e. both ends should keep the TCP connection open — until one of the parties sends a Connection: close header. But even though the connection is kept open, the client should still wait for the server response before sending in another request.\n\n\n\nHTTP pipelining is a technique that attempts to improve performance by breaking this rule. If an HTTP client supports pipelining it will go ahead and send several requests on the same connection without waiting for the corresponding responses. The server must also support pipelining and must send back the responses in a FIFO fashion.\n\nIf both ends support pipelining, then for each consecutive request you save the equivalent time of an extra roundtrip and if your application is able to process both requests in parallel you save that time as well.\n\nIf the server doesn't support pipelining, or worse, isn't aware of it, mixed responses can lead to weird and hard to track bugs. That is why most browsers have disabled pipelining. But this doesn't need to be an issue for our friendly reverse-proxy.\n\nA big concern however with pipelining is head-of-line blocking. A first, slower request, will stall flushing out responses to second faster requests. If the server processes all requests in parallel, the results from all the requests need to wait for the first one to complete so they can be sent back in the same order their respective requests came in. This means more buffering, higher memory usage, and with enough connections and requests it's easy to make the server run out of memory. In other words, it means a possible DoS attack vector:\n\n\nOpen and keep alive as many TCP connections as possible  \nFor each connection, start by sending an HTTP request to a known-to-be slow endpoint  \nWithout waiting for any server response send as much known-to-be fast requests as possible\n\n\nThis scary problem can be mitigated by:\n\n\nlimiting the maximum number of simultaneously handled pipelined requests\nsetting a timeout for every request and replying with a 408\n\n\nSo again, this might not be much of a concern either.\n\nDoes Node support HTTP pipelining?\n\nTo test this, we can tweak server.js a bit.\n\nconst http = require('http');  \nconst util = require('util');\n\nvar reqCounter = 0;  \nhttp.createServer((req, res) =&gt; {  \n  var reqId = ++reqCounter;\n  var start = clock();\n  setTimeout(reply, 3000);\n\n  function reply() {\n    var finish = clock();\n    var response = util.format('ID: %s Url: %s In: %s Out: %s',\n                               reqId, req.url, start, finish);\n\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    res.end(response);\n  }\n}).listen(8888);\n\nfunction clock() {  \n  return new Date().toTimeString().match(/\\S+/)[0];\n}\n\n\nEach request will now take 3 seconds and the response now includes the clock values of when the request started being processed and of when the reply was sent.\n\nNow let's fabricate a request payload with two requests in a reqs.txt file.\n\nGET /a HTTP/1.1  \nHost: localhost:8888  \nAccept: */*\n\n\nGET /b HTTP/1.1  \nHost: localhost:8888  \nAccept: */*\n\n\nThen spin up server.js and hit it with:\n\n$ tail -f req.txt | nc 127.0.0.1 8888\nHTTP/1.1 200 OK  \nContent-Type: text/plain  \nDate: Wed, 03 Feb 2016 16:23:39 GMT  \nConnection: keep-alive  \nTransfer-Encoding: chunked\n\n28  \nID: 1 Url: /a In: 16:23:36 Out: 16:23:39  \n0\n\nHTTP/1.1 200 OK  \nContent-Type: text/plain  \nDate: Wed, 03 Feb 2016 16:23:39 GMT  \nConnection: keep-alive  \nTransfer-Encoding: chunked\n\n28  \nID: 2 Url: /b In: 16:23:36 Out: 16:23:39  \n0\n\n\nThe clock values in both responses indicate that both requests were processed at the same time, and the request id and paths in the responses show that they were sent in the correct order. So Node does indeed handle pipelining correctly. The next question is...\n\nHow many requests will Node try to handle at the same time?\n\nLet's add a counter for the number of parallel requests and monitor it.\n\nconst http = require('http');  \nconst util = require('util');\n\nvar numReqs = 0;  \nhttp.createServer((req, res) =&gt; {  \n  ++numReqs;\n  setTimeout(reply, 50);\n  function reply() {\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    res.end('Hello world!');\n    --numReqs;\n  }\n}).listen(8888);\n\nsetInterval(() =&gt; {  \n  console.log(numReqs);\n}, 1000);\n\n\nRunning the same test, our server correctly reports 2 simultaneous requests. Let's see how high that number goes with wrk2, using a single connection.\n\n$  node server.js &amp;; pid=$!; sleep 1; wrk2 -R 50000 -d 5s -t 1 -c 1 http://localhost:8888/; kill -HUP $pid\n[1] 66128\nRunning 5s test @ http://localhost:8888/  \n  1 threads and 1 connections\n1  \n1  \n1  \n1  \n1  \n1  \n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.02s     0.00us   3.02s     0.00%\n    Req/Sec        nan       nan   0.00      0.00%\n  1 requests in 6.02s, 184.00B read\n  Socket errors: connect 0, read 0, write 0, timeout 2\nRequests/sec:      0.17  \nTransfer/sec:      30.56B  \n\n\nNever more than one. This means that wrk2 does not support HTTP pipelining, even though it reuses HTTP connections keeping them open, it waits for a reply before sending in another request.\n\nBut we still haven't reached our answer, how many requests will Node try to handle simultaneously for a single connection? It seems wrk2 is not going to be useful, so we better write our own small script.\n\nconst net = require('net');  \nconst payload =  \n  'GET / HTTP/1.1\\n' +\n  'Host: localhost:8888\\n' +\n  'Accept: */*\\n' +\n  '\\n\\n';\n\nvar socket = new net.Socket();  \nsocket.connect(8888, '127.0.0.1', function() {  \n  var c = 500 * 1000;\n  while (c--)\n    socket.write(payload);\n  socket.end('\\n');\n});\n\n\nThis will connect to port 8888 and shove half a million HTTP requests without waiting for any answers. Running against our 50ms delayed endpoint, the server now reports a much larger number of requests being handled simultaneously.\n\n$ node server.js\n0  \n0  \n0  \n3796  \n12817  \n2394  \n0  \n0  \n\n\nLet's change server.js again. Instead of a fixed delay let's make it random, that'll be a bit more realistic.\n\nconst http = require('http');  \nconst util = require('util');\n\nvar numReqs = 0;  \nhttp.createServer((req, res) =&gt; {  \n  ++numReqs;\n  setTimeout(reply, Math.floor(Math.random() * 1000));\n  function reply() {\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    res.end('Hello world!');\n    --numReqs;\n  }\n}).listen(8888);\n\nsetInterval(() =&gt; { console.log(numReqs); }, 1000);  \n\n\nUsing our custom client again, this is the kind of logs you get from the server:\n\n$ node server.js\n12044  \n527  \n2924  \n27672  \n71  \n21869  \n1  \n12382  \n\n\nNow the number of HTTP requests being handled simultaneously each second varies wildly.\n\nWhy is that? What is causing this throttling?\n\nIn Node, sockets are streams, and streams have an undocumented 'pause' event. So we could tap into this event on both sides of the connection.\n\nIn client.js, we add:\n\n// ...\nsocket.on('pause', console.trace);  \n\n\nIn server.js it gets a bit trickier:\n\n// ...\nhttp.createServer((req, res) =&gt; {  \n  var socket = req.socket;\n  if (!socket.tagged)\n    socket.on('pause', console.trace);\n  socket.tagged = true;\n\n  ++numReqs;\n// ...\n\n\n\n  The tagged flag added to the socket prevents registering a handler for the 'pause' event on the same socket object multiple times.\n\n\nRunning client and server again shows that the socket is being paused on the server end, and the stack trace says it's always coming from the same place:\n\nTrace  \n    at emitNone (events.js:72:20)\n    at Socket.emit (events.js:166:7)\n    at Socket.Readable.pause (_stream_readable.js:733:10)\n    at HTTPParser.parserOnIncoming [as onIncoming] (_http_server.js:468:16)\n    at HTTPParser.parserOnHeadersComplete (_http_common.js:88:23)\n\n\nDigging through the Node.js source code, we can see what’s pausing the stream:\n\n// ...\n  function parserOnIncoming(req, shouldKeepAlive) {\n    incoming.push(req);\n\n    // If the writable end isn't consuming, then stop reading\n    // so that we don't become overwhelmed by a flood of\n    // pipelined requests that may never be resolved.\n    if (!socket._paused) {\n      var needPause = socket._writableState.needDrain ||\n          outgoingData &gt;= socket._writableState.highWaterMark;\n      if (needPause) {\n        socket._paused = true;\n        // We also need to pause the parser, but don't do that until after\n        // the call to execute, because we may still be processing the last\n        // chunk.\n        socket.pause();\n      }\n    }\n// ...\n\n\nThe trigger is either socket._writableState.needDrain or outgoingData &gt;= socket._writableState.highWaterMark. We can confirm which one it is by printing the first value in server.js:\n\n// ...\nhttp.createServer((req, res) =&gt; {  \n  var socket = req.socket;\n  if (!socket.tagged)\n    socket.on('pause', () =&gt; { console.log(socket._writableState.needDrain) });\n  socket.tagged = true;\n\n  ++numReqs;\n// ...\n\n\nWe run it again, check the logs, and all we get is false. That means, unless we have a slow client, the only criteria that's stopping a Node HTTP server from processing more pipelined requests is outgoingData &gt;= socket._writableState.highWaterMark. The outgoingData variable holds the total byte size of the responses that are waiting to be flushed, responses that may be waiting for an earlier and slower request that's causing head-of-line blocking.\n\nWe even try and mess with socket._writableState.highWaterMark whose default value is 16kb:\n\n// ...\nhttp.createServer((req, res) =&gt; {  \n  var socket = req.socket;\n  if (!socket.tagged) {\n    socket._writableState.highWaterMark = Infinity;\n    socket.on('pause', () =&gt; { console.log(socket._writableState.needDrain) });\n  }\n  socket.tagged = true;\n\n  ++numReqs;\n// ...\n\n\nLet's see how it fares now:\n\n$ node server.js\n0  \n0  \n7930  \n15202  \n0  \n36932  \n0  \n64977  \n0  \n22486  \n956  \n0  \n38634  \n0  \n23995  \n234  \n0  \n13654  \n0  \n\n\nWe get higher numbers, but the server gets so flooded with requests that it still can't answer all of them without any hiccups. CPU usage was maxed out, and after a couple of seconds the process was using 1.4 GBs of memory. This should show how important it is to limit simultaneous requests and having response timeouts when HTTP pipelining is enabled.\n\nConclusion\n\nWhat did we learn?\n\n\nNode supports HTTP pipelining by default — the only limit is the size of the requests being head-of-line blocked\nHTTP pipelining can be a big benefit but it can also be a big risk if you don't manage it properly\n\n\nIf you have a Node service in these circumstances:\n\n\nsitting behind a load-balancer or a reverse proxy\nservice clients are experiencing big latencies/timeouts\nthe resources in the hosts running Node aren't saturated (cpu, mem, net)\nbackend services or databases (used by your Node service) aren't saturated\n\n\nThen your performance bottleneck is the number of connections between the load-balancer and Node. The available options to improve that bottleneck are:\n\n\nincreasing the number of connections\nenabling HTTP pipelining, but limiting it to prevent DoS\nmoving to HTTP/2\n\n\nHTTP/2 is able to multiplex different requests through the same connection avoiding head-of-line blocking, so if you're using it you have the best of both worlds — parallelised request handling and no head-of-line blocking."},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"},{"xmlns:media":"http://search.yahoo.com/mrss/"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"YLD Software Engineering Blog","description":"From software engineering expertise to Node.js news, events and company updates.","date":"2016-06-02T22:46:21.000Z","pubdate":"2016-06-02T22:46:21.000Z","pubDate":"2016-06-02T22:46:21.000Z","link":"http://blog.yld.io/","xmlurl":"http://blog.yld.io/rss/","xmlUrl":"http://blog.yld.io/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.7","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"YLD Software Engineering Blog"},"rss:description":{"@":{},"#":"From software engineering expertise to Node.js news, events and company updates."},"rss:link":{"@":{},"#":"http://blog.yld.io/"},"rss:generator":{"@":{},"#":"Ghost 0.7"},"rss:lastbuilddate":{"@":{},"#":"Thu, 02 Jun 2016 22:46:21 GMT"},"atom:link":{"@":{"href":"http://blog.yld.io/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}}]